{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googlesearch import search\n",
    "from newspaper import Article\n",
    "from newsfetch.news import newspaper\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import jsonlines\n",
    "from googleapiclient.discovery import build\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from urllib.parse import urlparse, urljoin\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import jsonlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = \"AIzaSyAC2fKvrczedZhBfZmSmYDRaYvSF_nm5HU\"\n",
    "SEID = \"017236688157949926015:-2mk2err3ps\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all the facts\n",
    "all_facts = []\n",
    "with jsonlines.open(\"/home/zsarwar/NLP/autoprompt/data/correctly_classified_roberta_large_autoprompt_format_shorter.jsonl\", 'r') as in_file:\n",
    "    for fact in in_file:\n",
    "        af = fact['Pre_Mask'] + fact['Label'] + fact['Post_Mask'].replace(\" .\", \".\")\n",
    "        all_facts.append(af)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def google_search_api(search_term, api_key, cse_id, num_results,num_iters,\n",
    "                       **kwargs):\n",
    "    start_index = -9\n",
    "    results = []\n",
    "    for i in range(num_iters):\n",
    "        if i == (num_iters - 1):\n",
    "            if num_iters == 10:\n",
    "                num_results = (num_results - 1)\n",
    "        start_index += 10\n",
    "        service = build(\"customsearch\", \"v1\", developerKey=api_key)\n",
    "        res = service.cse().list(q=search_term, cx=cse_id,num = num_results,start = start_index , **kwargs).execute()\n",
    "        results.append(res)\n",
    "    return results\n",
    "def get_urls_api(search_results):\n",
    "    extracted_urls = set()\n",
    "    for api_results in search_results:\n",
    "        for page in api_results:\n",
    "            if (page.get('items') is not None):\n",
    "                for i in range(len(page['items'])):\n",
    "                    ex = page['items'][i]['link']\n",
    "                    extracted_urls.add(ex)\n",
    "    return extracted_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 3/8 [00:20<00:33,  6.63s/it]connection/timeout error: https://www.tripadvisor.com/Hotel_Review-g1063524-d310064-Reviews-Hotel_Marad-Torre_Del_Greco_Province_of_Naples_Campania.html HTTPSConnectionPool(host='www.tripadvisor.com', port=443): Read timed out. (read timeout=6)\n",
      "100%|██████████| 8/8 [02:04<00:00, 15.61s/it]\n"
     ]
    }
   ],
   "source": [
    "for idx, fact in enumerate(all_facts[0:10]):\n",
    "  out_file = jsonlines.open(\"Testing_stuff_c.jsonl\", 'a')\n",
    "  search_results = []\n",
    "  api_results =  google_search_api(search_term = fact, api_key = API_KEY,\n",
    "                                  cse_id = SEID,\n",
    "                                  num_results = 5,num_iters = 1)\n",
    "  search_results.append(api_results)\n",
    "  extracted_urls = get_urls_api(search_results)\n",
    "  # Push wikipedia to the top (if it exists)\n",
    "  extracted_urls = list(extracted_urls)\n",
    "  for i, url in enumerate(extracted_urls):\n",
    "    if \"wikipedia\" in url:\n",
    "      extracted_urls.insert(0, extracted_urls.pop(i))      \n",
    "  article_num = 0\n",
    "  all_articles = {}\n",
    "  all_urls = {}\n",
    "  for link in tqdm(extracted_urls):\n",
    "    news = newspaper(link)\n",
    "    if (len(news.article.split(' ')) <= 100):\n",
    "      continue\n",
    "    else:\n",
    "      art = news.get_dict['article']\n",
    "      # Cleaning the article\n",
    "      art_split = art.split('.')\n",
    "      art_len = len(art_split) - 1\n",
    "      for i, sent in enumerate(reversed(art_split)):\n",
    "          if('disambiguation' in sent.lower() or 'redirects' in sent.lower()):\n",
    "              art_split.pop(art_len - i)      \n",
    "      art = '. '.join(art_split)\n",
    "      # Removing wikipedia based artifacts\n",
    "      art = re.sub(' ?\\[[0-9]*\\]', \"\", art)\n",
    "      art = re.sub('\\..{1,40}\\[ edit \\]', \".\", art)\n",
    "      art = re.sub('\\[ edit \\]', \"\", art)\n",
    "      all_articles[f'article_{article_num}'] = art\n",
    "      all_urls[f'URL_{article_num}'] = link\n",
    "      article_num+=1  \n",
    "  out_dict = {\"Index\" : idx, \"Fact\" : fact, \"URLS\" : all_urls, 'Articles': all_articles}\n",
    "  out_file.write(out_dict)\n",
    "  out_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('nlp2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "37239dad5b8b56b0d6556e681c63cac8a09607160e7510acb3444e24aad22957"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
