{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TRANSFORMERS_CACHE=/bigstor/zsarwar/models/cache\n",
      "env: CUDA_VISIBLE_DEVICES=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zsarwar/.conda/envs/nlp2/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%set_env TRANSFORMERS_CACHE=/bigstor/zsarwar/models/cache\n",
    "%set_env CUDA_VISIBLE_DEVICES=0\n",
    "import time\n",
    "import argparse\n",
    "import json\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import transformers\n",
    "from transformers import AutoConfig, AutoTokenizer, AutoModelWithLMHead, AutoModelForMaskedLM\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from tqdm import tqdm\n",
    "from nltk import tokenize\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "import nltk \n",
    "\n",
    "NER = spacy.load(\"en_core_web_sm\")\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.getLogger(\"transformers.tokenization_utils\").setLevel(logging.ERROR)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--label-map', type=str, default=None, help='JSON object defining label map')\n",
    "# LAMA-specific\n",
    "parser.add_argument('--tokenize-labels', action='store_true',\n",
    "                help='If specified labels are split into word pieces.'\n",
    "                        'Needed for LAMA probe experiments.')\n",
    "parser.add_argument('--filter', action='store_true', default=True,\n",
    "                help='If specified, filter out special tokens and gold objects.'\n",
    "                        'Furthermore, tokens starting with capital '\n",
    "                        'letters will not appear in triggers. Lazy '\n",
    "                        'approach for removing proper nouns.')\n",
    "parser.add_argument('--print-lama', action='store_true',\n",
    "                help='Prints best trigger in LAMA format.')\n",
    "parser.add_argument('--logfile', type=str, default='debug_jupyter')\n",
    "parser.add_argument('--initial-trigger', nargs='+', type=str, default=None, help='Manual prompt')\n",
    "parser.add_argument('--label-field', type=str, default='Prediction',\n",
    "                help='Name of the label field')\n",
    "parser.add_argument('--bsz', type=int, default=1, help='Batch size')\n",
    "parser.add_argument('--eval-size', type=int, default=1, help='Eval size')\n",
    "parser.add_argument('--iters', type=int, default=1,\n",
    "                help='Number of iterations to run trigger search algorithm')\n",
    "parser.add_argument('--accumulation-steps', type=int, default=1)\n",
    "parser.add_argument('--seed', type=int, default=0)\n",
    "parser.add_argument('--limit', type=int, default=None)\n",
    "parser.add_argument('--use-ctx', action='store_true',\n",
    "                help='Use context sentences for relation extraction only')\n",
    "parser.add_argument('--perturbed', action='store_true',\n",
    "                help='Perturbed sentence evaluation of relation extraction: replace each object in dataset with a random other object')\n",
    "parser.add_argument('--patience', type=int, default=5)\n",
    "\n",
    "parser.add_argument('--sentence-size', type=int, default=50)\n",
    "parser.add_argument('--debug', action='store_true')\n",
    "\n",
    "# Arguments needed in bashfile\n",
    "parser.add_argument('--train', type=Path, default='/home/zsarwar/NLP/autoprompt/data/datasets/final/correctly_classified_bert_large_cased_autoprompt_format_single_entity_500.jsonl', help='Train data path')\n",
    "parser.add_argument('--template', type=str,default='<s>{Pre_Mask}[P]{Post_Mask}[T]</s>', help='Template string')\n",
    "parser.add_argument('--filtered_vocab', type=str, default=None, help='JSON object defining label map')\n",
    "parser.add_argument('--model-name', type=str, default='bert-large-cased')\n",
    "parser.add_argument('--include_gpt', type=bool, default=True)\n",
    "parser.add_argument('--include_adv_token', type=bool, default=True )\n",
    "parser.add_argument('--include_wikipedia_padding', type=bool, default=False )\n",
    "parser.add_argument('--remove_periods', type=bool, default=True)\n",
    "parser.add_argument('--num-cand', type=int, default=10)\n",
    "\n",
    "\n",
    "#/home/zsarwar/NLP/autoprompt/data/datasets/final/correctly_classified_bert_large_cased_autoprompt_format_single_entity_500.jsonl\n",
    "#/home/zsarwar/NLP/autoprompt/data/datasets/final/correctly_classified_roberta_large_single_entity_500.jsonl\n",
    "\n",
    "args = parser.parse_args([])\n",
    "if args.debug:\n",
    "        level = logging.DEBUG\n",
    "else:\n",
    "        level = logging.INFO\n",
    "logfile = \"/home/zsarwar/NLP/autoprompt/autoprompt/Results/\"+ str(args.train).split(\"/\")[-1].split(\".\")[0]  +  \"_\" + args.logfile    \n",
    "numpy_file = \"/home/zsarwar/NLP/autoprompt/autoprompt/Results/Arrays/\" + str(args.train).split(\"/\")[-1].split(\".\")[0]  +  \"_\" + args.logfile + \".npy\"\n",
    "\n",
    "logging.basicConfig(filename=logfile,level=level)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'roberta' in args.model_name:\n",
    "    args.template = \"<s>{Pre_Mask}[P]{Post_Mask}[T]</s>\"\n",
    "    if(args.remove_periods):\n",
    "        import utils_v4_no_periods_roberta as utils_v4\n",
    "    else:\n",
    "        import utils_v4\n",
    "elif 'bert' in args.model_name:\n",
    "    args.template = \"[CLS]{Pre_Mask}[P]{Post_Mask}[T][SEP]\"\n",
    "    if(args.remove_periods):\n",
    "        import utils_v4_no_periods_bert as utils_v4\n",
    "    else:\n",
    "        import utils_v4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GradientStorage:\n",
    "    \"\"\"\n",
    "    This object stores the intermediate gradients of the output a the given PyTorch module, which\n",
    "    otherwise might not be retained.\n",
    "    \"\"\"\n",
    "    def __init__(self, module):\n",
    "        self._stored_gradient = None\n",
    "        module.register_full_backward_hook(self.hook)\n",
    "        # module.register_backward_hook(self.hook)\n",
    "\n",
    "    def hook(self, module, grad_in, grad_out):\n",
    "        self._stored_gradient = grad_out[0]\n",
    "\n",
    "    def get(self):\n",
    "        return self._stored_gradient\n",
    "\n",
    "class PredictWrapper:\n",
    "    \"\"\"\n",
    "    PyTorch transformers model wrapper. Handles necc. preprocessing of inputs for triggers\n",
    "    experiments.\n",
    "    \"\"\"\n",
    "    def __init__(self, model):\n",
    "        self._model = model\n",
    "\n",
    "    def __call__(self, model_inputs, trigger_ids):\n",
    "        model_inputs = model_inputs.copy()\n",
    "        trigger_mask = model_inputs.pop('trigger_mask')\n",
    "        model_inputs = replace_trigger_tokens(model_inputs, trigger_ids, trigger_mask)\n",
    "        predict_mask = model_inputs.pop('predict_mask')\n",
    "        logits = self._model(**model_inputs).logits\n",
    "        predict_logits = logits.masked_select(predict_mask.unsqueeze(-1)).view(logits.size(0), -1)\n",
    "        return predict_logits, model_inputs\n",
    "\n",
    "class AccuracyFn:\n",
    "    \"\"\"\n",
    "    Computing the accuracy when a label is mapped to multiple tokens is difficult in the current\n",
    "    framework, since the data generator only gives us the token ids. To get around this we\n",
    "    compare the target logp to the logp of all labels. If target logp is greater than all (but)\n",
    "    one of the label logps we know we are accurate.\n",
    "    \"\"\"\n",
    "    def __init__(self, tokenizer, label_map, device, tokenize_labels=False):\n",
    "        self._all_label_ids = []\n",
    "        self._pred_to_label = []\n",
    "        logger.info(label_map)\n",
    "        for label, label_tokens in label_map.items():\n",
    "            self._all_label_ids.append(utils.encode_label(tokenizer, label_tokens, tokenize_labels).to(device))\n",
    "            self._pred_to_label.append(label)\n",
    "        logger.info(self._all_label_ids)\n",
    "\n",
    "    def __call__(self, predict_logits, gold_label_ids):\n",
    "        # Get total log-probability for the true label\n",
    "        gold_logp = get_loss(predict_logits, gold_label_ids)\n",
    "\n",
    "        # Get total log-probability for all labels\n",
    "        bsz = predict_logits.size(0)\n",
    "        all_label_logp = []\n",
    "        for label_ids in self._all_label_ids:\n",
    "            label_logp = get_loss(predict_logits, label_ids.repeat(bsz, 1))\n",
    "            all_label_logp.append(label_logp)\n",
    "        all_label_logp = torch.stack(all_label_logp, dim=-1)\n",
    "        _, predictions = all_label_logp.max(dim=-1)\n",
    "        predictions = [self._pred_to_label[x] for x in predictions.tolist()]\n",
    "\n",
    "        # Add up the number of entries where loss is greater than or equal to gold_logp.\n",
    "        ge_count = all_label_logp.le(gold_logp.unsqueeze(-1)).sum(-1)\n",
    "        correct = ge_count.le(1)  # less than in case of num. prec. issues\n",
    "\n",
    "        return correct.float()\n",
    "\n",
    "    # TODO: @rloganiv - This is hacky. Replace with something sensible.\n",
    "    def predict(self, predict_logits):\n",
    "        bsz = predict_logits.size(0)\n",
    "        all_label_logp = []\n",
    "        for label_ids in self._all_label_ids:\n",
    "            label_logp = get_loss(predict_logits, label_ids.repeat(bsz, 1))\n",
    "            all_label_logp.append(label_logp)\n",
    "        all_label_logp = torch.stack(all_label_logp, dim=-1)\n",
    "        _, predictions = all_label_logp.max(dim=-1)\n",
    "        predictions = [self._pred_to_label[x] for x in predictions.tolist()]\n",
    "        return predictions\n",
    "\n",
    "\n",
    "def load_pretrained(model_name):\n",
    "    \"\"\"\n",
    "    Loads pretrained HuggingFace config/model/tokenizer, as well as performs required\n",
    "    initialization steps to facilitate working with triggers.\n",
    "    \"\"\"\n",
    "    config = AutoConfig.from_pretrained(model_name)\n",
    "    model = AutoModelForMaskedLM.from_pretrained(model_name)\n",
    "    model.eval()\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, add_prefix_space=True)\n",
    "    utils_v4.add_task_specific_tokens(tokenizer)\n",
    "    return config, model, tokenizer\n",
    "\n",
    "def set_seed(seed: int):\n",
    "    \"\"\"Sets the relevant random seeds.\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.random.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "def get_embeddings(model, config):\n",
    "    \"\"\"\n",
    "    Returns the wordpiece embedding module.\n",
    "    \"\"\"\n",
    "    base_model = getattr(model, config.model_type)\n",
    "    embeddings = base_model.embeddings.word_embeddings\n",
    "    return embeddings\n",
    "\n",
    "def compute_accuracy(predict_logits, labels):\n",
    "    target_logp = F.log_softmax(predict_logits, dim=-1)\n",
    "    max_pred = torch.argmax(target_logp, dim=-1).unsqueeze(-1)\n",
    "    mask = max_pred.eq(labels)\n",
    "    correct = mask.nonzero().shape[0]\n",
    "    total = labels.shape[0]\n",
    "    acc = correct / total\n",
    "    return correct\n",
    "\n",
    "def hotflip_attack(averaged_grad,\n",
    "                   normalized_embedding_matrix,\n",
    "                   increase_loss=False,\n",
    "                   num_candidates=1,\n",
    "                   filter=None):\n",
    "    \"\"\"Returns the top candidate replacements.\"\"\"\n",
    "    with torch.no_grad():\n",
    "        gradient_dot_embedding_matrix = torch.matmul(\n",
    "            normalized_embedding_matrix,\n",
    "            averaged_grad\n",
    "        )\n",
    "\n",
    "        if filter is not None:\n",
    "            gradient_dot_embedding_matrix -= filter\n",
    "            \n",
    "        if not increase_loss:\n",
    "            gradient_dot_embedding_matrix *= -1\n",
    "\n",
    "    _, top_k_ids = gradient_dot_embedding_matrix.topk(num_candidates)\n",
    "    return top_k_ids\n",
    "\n",
    "def get_pred_label(predict_logits, labels, tokenizer):\n",
    "    target_logp = F.log_softmax(predict_logits, dim=-1)\n",
    "    max_pred = torch.argmax(target_logp, dim=-1).unsqueeze(-1)\n",
    "    return max_pred\n",
    "\n",
    "def get_loss(predict_logits, label_ids):\n",
    "    predict_logp = F.log_softmax(predict_logits, dim=-1)\n",
    "    target_logp = predict_logp.gather(-1, label_ids)\n",
    "    target_logp = target_logp - 1e32 * label_ids.eq(0)  # Apply mask\n",
    "    target_logp = torch.logsumexp(target_logp, dim=-1)\n",
    "    return -target_logp\n",
    "\n",
    "def isVariable(idx, tokenizer, allowed_words):\n",
    "    word = tokenizer.decode([idx])\n",
    "    word = word.replace(\" \", \"\")\n",
    "    _isVar = False\n",
    "    upper_locs = [i for i, ch in enumerate(word) if ch.isupper()]\n",
    "    # Check if caps in between and entire word is not upper-case\n",
    "    if(len(upper_locs) > 0 and len(upper_locs) < len(word)):\n",
    "        for idx in upper_locs:\n",
    "            if (idx > 0):\n",
    "            # Check if token is not real entity like McDonalds                \n",
    "                parsed_word= NER(word)\n",
    "                if (len(parsed_word.ents) == 0):\n",
    "                    if(word not in allowed_words):\n",
    "                        _isVar = True\n",
    "                    break \n",
    "    return _isVar\n",
    "\n",
    "def is_all_capps_or_num(idx, tokenizer):\n",
    "    word = tokenizer.decode([idx])\n",
    "    word = word.replace(\" \", \"\")\n",
    "    _is_all_caps_nums = False\n",
    "    word_upper = word.upper()\n",
    "    if(word_upper == word):\n",
    "        _is_all_caps_nums = True\n",
    "    # Check if it contains a number    \n",
    "    if (any(char.isdigit() for char in word)):\n",
    "        _is_all_caps_nums = True\n",
    "    return _is_all_caps_nums\n",
    "\n",
    "\n",
    "def replace_trigger_tokens(model_inputs, trigger_ids, trigger_mask):\n",
    "    out = model_inputs.copy()    \n",
    "    # Count number of false values\n",
    "    new_len = (torch.count_nonzero(trigger_mask.eq(False)) + trigger_ids.shape[1]).item()\n",
    "    # New trigger mask\n",
    "    new_trigger_mask = torch.zeros(new_len, dtype=torch.bool, device=device).unsqueeze(0)\n",
    "    # Get index of first true element in the old mask and fill in new_trigger_mask\n",
    "    trigger_start_index = torch.where(trigger_mask == True)[1][0].item()\n",
    "    new_trigger_mask[0][trigger_start_index: trigger_start_index + trigger_ids.shape[1]] = True\n",
    "    # New input_ids_tensor\n",
    "    new_input_ids = torch.full(new_trigger_mask.shape, fill_value=-1, device=device)\n",
    "    # Fill in og ids\n",
    "    og_text_ids = (torch.masked_select(out['input_ids'], trigger_mask.eq(False)))\n",
    "    new_input_ids.masked_scatter_(new_trigger_mask.eq(False), og_text_ids)\n",
    "    # Fill in new trigger_ids\n",
    "    new_input_ids.masked_scatter_(new_trigger_mask, trigger_ids)\n",
    "    # New prediction mask\n",
    "    new_pred_mask = torch.full(new_trigger_mask.shape, fill_value=0, device=device,dtype=torch.bool)\n",
    "    # Need to check for number of trigger tokens in both masks\n",
    "    if(\"token_type_ids\" in out):\n",
    "        new_tok_type_ids = torch.zeros(new_trigger_mask.shape, device=device, dtype=torch.int32)\n",
    "        out['token_type_ids'] = new_tok_type_ids\n",
    "    pred_mask_true_index = torch.where(out['predict_mask'])[1][0].item()\n",
    "    num_trig_tokens_old = torch.count_nonzero(trigger_mask)\n",
    "    num_trig_tokens_new = torch.count_nonzero(new_trigger_mask)\n",
    "    diff = num_trig_tokens_new - num_trig_tokens_old\n",
    "    if(trigger_start_index > pred_mask_true_index):\n",
    "        # Copy/paste into the same index as is\n",
    "        new_pred_mask[0][pred_mask_true_index] = True\n",
    "    else:\n",
    "        new_pred_mask[0][pred_mask_true_index + diff] = True\n",
    "    # Finally, a new attention mask is also needed\n",
    "    new_attention_mask = torch.full(new_input_ids.shape, fill_value=1, device=device)\n",
    "    out['input_ids'] = new_input_ids\n",
    "    out['predict_mask'] = new_pred_mask\n",
    "    out['attention_mask'] = new_attention_mask    \n",
    "    return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]Gerakan Pramuka Indonesia was employed in[P][T][SEP]\n",
      "[CLS]Balbinus expired at[P][T][SEP]\n",
      "[CLS][P]was the language of Ram Rajya[T][SEP]\n",
      "[CLS]iPod shuffle, developed by[P][T][SEP]\n",
      "[CLS]as a citizen of[P], Philippe Leclerc de Hauteclocque[T][SEP]\n",
      "[CLS]as a citizen of[P], Priyanka Vadra[T][SEP]\n",
      "[CLS]The headquarters of Global Humanitarian Forum is in[P][T][SEP]\n",
      "[CLS]The headquarter of Sheffield Wednesday F.C. is in[P][T][SEP]\n",
      "[CLS]Luis del Sol, who plays in[P]position[T][SEP]\n",
      "[CLS]BBC Persian Television is owned by[P][T][SEP]\n",
      "[CLS]The headquarters of Crystal Palace Baltimore is in[P][T][SEP]\n",
      "[CLS]Google Trends is owned by[P][T][SEP]\n",
      "[CLS]Silvio Orlando used to communicate in[P][T][SEP]\n",
      "[CLS]The headquarter of Wayne State University Press is located in[P][T][SEP]\n",
      "[CLS][P]owns iTunes Radio[T][SEP]\n",
      "[CLS]Jean Sibelius took up work in[P][T][SEP]\n",
      "[CLS]Sheridan Morley died in[P][T][SEP]\n",
      "[CLS]Hermann Ebbinghaus works in the field of[P][T][SEP]\n",
      "[CLS]Viktor Chernomyrdin passed away in[P][T][SEP]\n",
      "[CLS]Nikolaus Dumba was born in[P][T][SEP]\n",
      "[CLS]Donkey Kong, a product created by[P][T][SEP]\n",
      "[CLS]Antonis Samaras has a citizenship of[P][T][SEP]\n",
      "[CLS]David Burliuk communicated in[P][T][SEP]\n",
      "[CLS]The headquarter of Csepel SC is located in[P][T][SEP]\n",
      "[CLS]Julia Lovell used the[P]language[T][SEP]\n",
      "[CLS]Lazio, which has the capital city[P][T][SEP]\n",
      "[CLS]bicycle wheel is a part of[P][T][SEP]\n",
      "[CLS]culinary art,  a type of[P][T][SEP]\n",
      "[CLS]Cape Penck is located in[P][T][SEP]\n",
      "[CLS]as a citizen of[P], Roisin McGettigan[T][SEP]\n",
      "[CLS]The capital of Kingdom of Italy is[P][T][SEP]\n",
      "[CLS]WaterAid is headquartered in[P][T][SEP]\n",
      "[CLS]bacterial pneumonia is a subclass of[P][T][SEP]\n",
      "[CLS]Bharat Agnihotri, an[P]-born person[T][SEP]\n",
      "[CLS]American wine is a subclass of[P][T][SEP]\n",
      "[CLS][P]musicians such as Kenny Werner[T][SEP]\n",
      "[CLS]Honda S2000 is created by[P][T][SEP]\n",
      "[CLS]The capital of Fatimid caliphate is[P][T][SEP]\n",
      "[CLS]El Oued Province can be found in[P][T][SEP]\n",
      "[CLS]Bruce County is located in[P][T][SEP]\n",
      "[CLS]Windows Embedded CE 6.0, a product of[P][T][SEP]\n",
      "[CLS]Resource Interchange File Format, created by[P][T][SEP]\n",
      "[CLS]The[P]-language creation Mir Fantastiki[T][SEP]\n",
      "[CLS]Toyota Porte, created by[P][T][SEP]\n",
      "[CLS]Naumburg (Saale) is located in[P][T][SEP]\n",
      "[CLS]Henry V of England is a citizen of[P][T][SEP]\n",
      "[CLS]Ferrari F430, produced by[P][T][SEP]\n",
      "[CLS]Kyōto Prefecture, located in[P][T][SEP]\n",
      "[CLS][P]musicians Miles Davis[T][SEP]\n",
      "[CLS]Dodge Coronet is produced by[P][T][SEP]\n",
      "[CLS]Aleksey Brusilov used to communicate in[P][T][SEP]\n",
      "[CLS]Raquel Morell used to communicate in[P][T][SEP]\n",
      "[CLS]Victor Sikora plays in the position of[P][T][SEP]\n",
      "[CLS]The mother tongue of Gough Whitlam is[P][T][SEP]\n",
      "[CLS]Randy Bachman, who has a citizenship of[P][T][SEP]\n",
      "[CLS]The language of Kladderadatsch was[P][T][SEP]\n",
      "[CLS]aorta,  a type of[P][T][SEP]\n",
      "[CLS]The mother tongue of Marguerite Audoux is[P][T][SEP]\n",
      "[CLS]Honda Odyssey, produced by[P][T][SEP]\n",
      "[CLS]The original language of Human Touch was[P][T][SEP]\n",
      "[CLS]Microsoft Access was a product of[P][T][SEP]\n",
      "[CLS]Arvo Tuominen used the[P]language[T][SEP]\n",
      "[CLS]Tripoli is the capital of[P][T][SEP]\n",
      "[CLS]Bartholomeus Breenbergh died in[P][T][SEP]\n",
      "[CLS]Rajesh Khanna used to communicate in[P][T][SEP]\n",
      "[CLS]The language of Le chalet was[P][T][SEP]\n",
      "[CLS]The mother tongue of Joseph Roumanille is[P][T][SEP]\n",
      "[CLS]The Sports Reporters debuted on[P][T][SEP]\n",
      "[CLS]Windows Server 2012 was created by[P][T][SEP]\n",
      "[CLS]The headquarter of Cambridge University Press is located in[P][T][SEP]\n",
      "[CLS]Manuel I of Portugal succumbed at[P][T][SEP]\n",
      "[CLS]Architectural Review, written in the[P]language[T][SEP]\n",
      "[CLS]The[P]-owned BBC Red Button[T][SEP]\n",
      "[CLS]Giovanni Lanfranco passed away at[P][T][SEP]\n",
      "[CLS]Eyeshield 21, that originated in[P][T][SEP]\n",
      "[CLS]Triumph TR2, a product of[P],[T][SEP]\n",
      "[CLS]Palladam is located in[P][T][SEP]\n",
      "[CLS]Oreste Biancoli communicated in[P][T][SEP]\n",
      "[CLS]Transwede Airways's headquarters are in[P][T][SEP]\n",
      "[CLS]The[P]language is the official language of Suriname[T][SEP]\n",
      "[CLS]Augusto Pinochet communicated in[P][T][SEP]\n",
      "[CLS][P], the creator of the Intel 80486DX2[T][SEP]\n",
      "[CLS]Buffet Crampon originated in[P][T][SEP]\n",
      "[CLS]Massimiliano Allegri plays in the position of[P][T][SEP]\n",
      "[CLS][P]is Patrice Leconte's native language[T][SEP]\n",
      "[CLS]Derby School is located in[P][T][SEP]\n",
      "[CLS]Mr Selfridge is located in[P][T][SEP]\n",
      "[CLS]Shelbourne F.C. is based in[P][T][SEP]\n",
      "[CLS]Tariq Abdul-Wahad follows the[P]religion[T][SEP]\n",
      "[CLS]Ferrari 125 F1, a product created by[P],[T][SEP]\n",
      "[CLS]Ricardo Faty, who plays in[P]position[T][SEP]\n",
      "[CLS]as a citizen of[P], Mario Monti[T][SEP]\n",
      "[CLS]Officially,[P]is the language of United States of America[T][SEP]\n",
      "[CLS]Emilia Rydberg was born in[P][T][SEP]\n",
      "[CLS]Iosif Rotariu plays as[P][T][SEP]\n",
      "[CLS]The official language of South Tyrol is[P][T][SEP]\n",
      "[CLS]Bartolomeo Ammannati lost their life at[P][T][SEP]\n",
      "[CLS]The headquarters of Smirnoff is in[P][T][SEP]\n",
      "[CLS]The language of The Atlantic is[P][T][SEP]\n",
      "[CLS]The headquarters of Institute of Chartered Secretaries and Administrators is in[P][T][SEP]\n",
      "[CLS]Airbus A310 is created by[P][T][SEP]\n",
      "[CLS]Margaret Forster communicated in[P][T][SEP]\n",
      "[CLS]Suleiman the Magnificent is affiliated with the[P]religion[T][SEP]\n",
      "[CLS]Microsoft Store is developed by[P][T][SEP]\n",
      "[CLS][P]debuted NFL on NBC[T][SEP]\n",
      "[CLS]Germany in the Eurovision Song Contest 2010 was created in[P][T][SEP]\n",
      "[CLS]Nissan President, a product created by[P],[T][SEP]\n",
      "[CLS]Tower Island belongs to the continent of[P][T][SEP]\n",
      "[CLS]The headquarter of Iranian reform movement is located in[P][T][SEP]\n",
      "[CLS]Christophe Barratier used the[P]language[T][SEP]\n",
      "[CLS]Volvo C70 is developed by[P][T][SEP]\n",
      "[CLS]Programmed Airline Reservations System is a product of[P][T][SEP]\n",
      "[CLS][P]is the official language of Iitti[T][SEP]\n",
      "[CLS]The[P]-language creation Pourvu qu'elles soient douces[T][SEP]\n",
      "[CLS]2000 Summer Olympics is in[P][T][SEP]\n",
      "[CLS]File Explorer, created by[P][T][SEP]\n",
      "[CLS][P], the creator of the iPhone 5[T][SEP]\n",
      "[CLS]as a citizen of[P], Rudolf Hess[T][SEP]\n",
      "[CLS]V. Shantaram expired at[P][T][SEP]\n",
      "[CLS]linen is a subclass of[P][T][SEP]\n",
      "[CLS]Florence Charterhouse is located in[P][T][SEP]\n",
      "[CLS]Abraham Isaac Kook used to communicate in[P][T][SEP]\n",
      "[CLS]The language of Mediterraneo was[P][T][SEP]\n",
      "[CLS]Dov Ber of Mezeritch is a[P]by training[T][SEP]\n",
      "[CLS]Rivaldo plays in the position of[P][T][SEP]\n",
      "[CLS]Sialkot district, which is located in[P][T][SEP]\n",
      "[CLS]Alfa Romeo 6C, produced by[P][T][SEP]\n",
      "[CLS]The mother tongue of Francis de Croisset is[P][T][SEP]\n",
      "[CLS]Carlos Fuentes used to communicate in[P][T][SEP]\n",
      "[CLS][P]musicians such as Bennie Wallace[T][SEP]\n",
      "[CLS]Phoenix Sky Harbor International Airport is located in[P][T][SEP]\n",
      "[CLS]Anatolia Eyalet's capital city,[P][T][SEP]\n",
      "[CLS]In Central African Republic,[P]is the official language[T][SEP]\n",
      "[CLS]The language of The Wardstone Chronicles was[P][T][SEP]\n",
      "[CLS]Ali Naqi Naqvi is follower of[P][T][SEP]\n",
      "[CLS]Hiro Matsushita, a citizen of[P][T][SEP]\n",
      "[CLS]Henry Manners, 8th Duke of Rutland found employment in[P][T][SEP]\n",
      "[CLS]Henry Martin Jackson communicated in[P][T][SEP]\n",
      "[CLS]Stara Zagora, which is located in[P][T][SEP]\n",
      "[CLS]The law in Guyana defines[P]as the official language[T][SEP]\n",
      "[CLS]Giurgiu County is located in[P][T][SEP]\n",
      "[CLS]Michael Hutchence communicated in[P][T][SEP]\n",
      "[CLS]Jean-Baptiste Willermoz used to communicate in[P][T][SEP]\n",
      "[CLS]The headquarter of Italian Democratic Socialist Party is located in[P][T][SEP]\n",
      "[CLS]Shlomo Avineri used to communicate in[P][T][SEP]\n",
      "[CLS]Eleanor Steber used the[P]language[T][SEP]\n",
      "[CLS]Manuel Valls communicated in[P][T][SEP]\n",
      "[CLS]D'elles is written in the[P]language[T][SEP]\n",
      "[CLS]Lancia Musa is developed by[P][T][SEP]\n",
      "[CLS]The headquarter of Shanghai Electric is located in[P][T][SEP]\n",
      "[CLS]Suzuki GSX-R750 is created by[P][T][SEP]\n",
      "[CLS]Mac OS X 10.1, developed by[P][T][SEP]\n",
      "[CLS]Shaan Shahid communicated in[P][T][SEP]\n",
      "[CLS]Annuario Pontificio, an[P]-language work[T][SEP]\n",
      "[CLS]Cape Rymill is located in the continent[P][T][SEP]\n",
      "[CLS]The headquarter of University of Paris is located in[P][T][SEP]\n",
      "[CLS]Moissac Abbey is located in[P][T][SEP]\n",
      "[CLS]Dwynwen, written in the[P]language[T][SEP]\n",
      "[CLS]Kathimerini, that originated in[P][T][SEP]\n",
      "[CLS]as a citizen of[P], Marcin Libicki[T][SEP]\n",
      "[CLS]obelisk is a part of[P][T][SEP]\n",
      "[CLS]Madeleine Robinson communicated in[P][T][SEP]\n",
      "[CLS]Kerlon Moura Souza plays in the position of[P][T][SEP]\n",
      "[CLS]Honda CR-V, created by[P][T][SEP]\n",
      "[CLS]Cistercian nuns is a subclass of[P][T][SEP]\n",
      "[CLS]Tygodnik Ilustrowany, a[P]-language work[T][SEP]\n",
      "[CLS]Nanyang Technological University is in[P][T][SEP]\n",
      "[CLS]Boeing Helicopters is owned by[P][T][SEP]\n",
      "[CLS]The language of The Main Chance is[P][T][SEP]\n",
      "[CLS]The capital of Bermuda is[P][T][SEP]\n",
      "[CLS]The headquarters of Citilink is in[P][T][SEP]\n",
      "[CLS]Officially,[P]is the language of Czechoslovakia[T][SEP]\n",
      "[CLS]The native language of Hamidou Benmassoud is[P][T][SEP]\n",
      "[CLS]Ellen Wilkinson used the[P]language[T][SEP]\n",
      "[CLS]Mohammed Noor plays in the position of[P][T][SEP]\n",
      "[CLS]Dodge Omni, a product created by[P],[T][SEP]\n",
      "[CLS]The language of...And Then There Were Three... was[P][T][SEP]\n",
      "[CLS]John Vanderbank was born in[P][T][SEP]\n",
      "[CLS]Delia Murphy has a citizenship of[P][T][SEP]\n",
      "[CLS]The law in Prince Edward Island declares[P]the official language[T][SEP]\n",
      "[CLS]J-pop was formed in[P][T][SEP]\n",
      "[CLS]Torstein Aagaard-Nilsen was born in[P][T][SEP]\n",
      "[CLS]The law in East Kalimantan declares[P]the official language[T][SEP]\n",
      "[CLS]The headquarters of Pantheon-Sorbonne University is in[P][T][SEP]\n",
      "[CLS]Thy-1 cell surface antigen,  a type of[P][T][SEP]\n",
      "[CLS]Officially,[P]is the language of Khanty-Mansi Autonomous Okrug[T][SEP]\n",
      "[CLS]Egypt Eyalet's capital,[P][T][SEP]\n",
      "[CLS]Peacock Sound is a part of the continent of[P][T][SEP]\n",
      "[CLS]Stansted Express is located in[P][T][SEP]\n",
      "[CLS]Cadillac STS Wheels is developed by[P][T][SEP]\n",
      "[CLS][P]is Munzir ibn Sawa Al Tamimi's official religion[T][SEP]\n",
      "[CLS]as a citizen of[P], Leif Sylvester Petersen[T][SEP]\n",
      "[CLS]limousine,  a type of[P][T][SEP]\n",
      "[CLS]Chevrolet Greenbrier, created by[P][T][SEP]\n",
      "[CLS]The Adventures of Tom Sawyer was a work in the[P]language[T][SEP]\n",
      "[CLS]Utah State Route 152 is located in[P][T][SEP]\n",
      "[CLS]as a citizen of[P], Soenario[T][SEP]\n",
      "[CLS]The headquarter of Lavalin is located in[P][T][SEP]\n",
      "[CLS]Patrick Chamoiseau communicated in[P][T][SEP]\n",
      "[CLS]Juho Kusti Paasikivi worked in[P][T][SEP]\n",
      "[CLS]as a citizen of[P], Rangsit Prayurasakdi[T][SEP]\n",
      "[CLS]Volvo 480 is created by[P][T][SEP]\n",
      "[CLS][P]musicians Bendik Hofseth[T][SEP]\n",
      "[CLS]The headquarters of Singapore Bus Service is in[P][T][SEP]\n",
      "[CLS]The native language of Gerrit Komrij is[P][T][SEP]\n",
      "[CLS]Acura EL, developed by[P][T][SEP]\n",
      "[CLS]Alexander Bulatovich communicated in[P][T][SEP]\n",
      "[CLS]Mapple Glacier belongs to the continent of[P][T][SEP]\n",
      "[CLS]Paulus Manutius took up work in[P][T][SEP]\n",
      "[CLS]Managua, the capital city of[P][T][SEP]\n",
      "[CLS]Toyota Publica, a product developed by[P],[T][SEP]\n",
      "[CLS]Yunjin Kim communicated in[P][T][SEP]\n",
      "[CLS]Sergey Lukyanenko used to work in[P][T][SEP]\n",
      "[CLS]iPhone 5 is produced by[P][T][SEP]\n",
      "[CLS]Paul Ramadier used the[P]language[T][SEP]\n",
      "[CLS]Arthur Balfour was employed in[P][T][SEP]\n",
      "[CLS]Thai Rath was a[P]-language work[T][SEP]\n",
      "[CLS][P]is the language used by Emmanuelle Bercot[T][SEP]\n",
      "[CLS]Edmonton Journal is in[P][T][SEP]\n",
      "[CLS]Bourg-la-Reine is located in[P][T][SEP]\n",
      "[CLS]The law in Germany declares[P]the official language[T][SEP]\n",
      "[CLS]Turkey national rugby union team, located in[P][T][SEP]\n",
      "[CLS]The original language of The Telegraph is[P][T][SEP]\n",
      "[CLS]statistical model specializes in[P][T][SEP]\n",
      "[CLS]Gildo Bocci used to communicate in[P][T][SEP]\n",
      "[CLS]Ludwig Thuille succumbed at[P][T][SEP]\n",
      "[CLS]The[P]language is the official language of Roman Empire[T][SEP]\n",
      "[CLS]William Osler, who holds a citizenship of[P][T][SEP]\n",
      "[CLS]The native language of Stieg Larsson is[P][T][SEP]\n",
      "[CLS]C'est dans l'air is a work in the[P]language[T][SEP]\n",
      "[CLS]The original language of La Chienne is[P][T][SEP]\n",
      "[CLS]The[P]-language creation Le Matin de Paris[T][SEP]\n",
      "[CLS]IBM 704 is developed by[P][T][SEP]\n",
      "[CLS]Euroclear, who was from[P][T][SEP]\n",
      "[CLS]Patrick Chinamasa, who has a citizenship of[P][T][SEP]\n",
      "[CLS]Lutvann, which is located in[P][T][SEP]\n",
      "[CLS]Kanchenjunga is a part of the continent of[P][T][SEP]\n",
      "[CLS]Pink Moon was an[P]-language work[T][SEP]\n",
      "[CLS]Arrasando is a[P]-language work[T][SEP]\n",
      "[CLS]Google Glass was a product of[P][T][SEP]\n",
      "[CLS]Phillips Brooks died in[P][T][SEP]\n",
      "[CLS]Carla Accardi died in[P][T][SEP]\n",
      "[CLS]Renault Estafette, produced by[P][T][SEP]\n",
      "[CLS]Pensacola Mountains belongs to the continent of[P][T][SEP]\n",
      "[CLS]Soppressata was formulated in[P][T][SEP]\n",
      "[CLS]The mother tongue of Louis Carrogis Carmontelle is[P][T][SEP]\n",
      "[CLS]2011 German Masters is located in[P][T][SEP]\n",
      "[CLS]B-29 Superfortress, created by[P][T][SEP]\n",
      "[CLS]Nissan MR engine is produced by[P][T][SEP]\n",
      "[CLS]The[P]-language creation Sina Weibo[T][SEP]\n",
      "[CLS]Islamabad, that is the capital city of[P][T][SEP]\n",
      "[CLS]The official language of Snezhinsk is[P][T][SEP]\n",
      "[CLS]The language of My Bloody Valentine 3D was[P][T][SEP]\n",
      "[CLS]Autauga County can be found in[P][T][SEP]\n",
      "[CLS]Claude Chappe used to communicate in[P][T][SEP]\n",
      "[CLS]Fiat Palio, produced by[P][T][SEP]\n",
      "[CLS]Antonino D'Agostino plays as[P][T][SEP]\n",
      "[CLS]Honkala Island is a part of the continent of[P][T][SEP]\n",
      "[CLS]California Dreams was originally aired on[P][T][SEP]\n",
      "[CLS]folding bicycle is a subclass of[P][T][SEP]\n",
      "[CLS]BMW M60 is created by[P][T][SEP]\n",
      "[CLS]iPad 3, a product of[P][T][SEP]\n",
      "[CLS]Yuan Shikai died at[P][T][SEP]\n",
      "[CLS][P]musicians such as Count Basie[T][SEP]\n",
      "[CLS]Samsun Province is in[P][T][SEP]\n",
      "[CLS][P]owns Google Images[T][SEP]\n",
      "[CLS]IBM 8100, a product created by[P],[T][SEP]\n",
      "[CLS]University of Geneva, whose headquarters are in[P][T][SEP]\n",
      "[CLS]Fiat Marea is produced by[P][T][SEP]\n",
      "[CLS]The mother tongue of Rajesh Khanna is[P][T][SEP]\n",
      "[CLS]IBM 4300 is a product of[P][T][SEP]\n",
      "[CLS]The[P]-language creation Jurassic Park[T][SEP]\n",
      "[CLS]phylogenetics is part of[P][T][SEP]\n",
      "[CLS]Napier Mountains belongs to the continent of[P][T][SEP]\n",
      "[CLS]The native language of Herman Gorter is[P][T][SEP]\n",
      "[CLS]Martin Burrell used the[P]language[T][SEP]\n",
      "[CLS]Scott Base belongs to the continent of[P][T][SEP]\n",
      "[CLS]Agatha Christie's Marple premiered on[P][T][SEP]\n",
      "[CLS]Santiago Segura used to communicate in[P][T][SEP]\n",
      "[CLS]Werner Heisenberg's occupation is[P][T][SEP]\n",
      "[CLS]Ivry Gitlis, who holds a citizenship of[P][T][SEP]\n",
      "[CLS]Krypteria was formed in[P][T][SEP]\n",
      "[CLS][P]is the language typically used by Philip V of Spain[T][SEP]\n",
      "[CLS]Germany is located in the continent[P][T][SEP]\n",
      "[CLS]Davey Lee is an[P]by training[T][SEP]\n",
      "[CLS][P], that is the capital city of South Australia[T][SEP]\n",
      "[CLS]Massimiliano Cappioli plays as[P][T][SEP]\n",
      "[CLS]Renin, which is a subclass of[P][T][SEP]\n",
      "[CLS]Satyendra Nath Bose communicated in[P][T][SEP]\n",
      "[CLS]The capital of Queensland is[P][T][SEP]\n",
      "[CLS]Willamette Valley AVA is located in[P][T][SEP]\n",
      "[CLS]Michael Winner communicated in[P][T][SEP]\n",
      "[CLS]The capital of Ireland is[P][T][SEP]\n",
      "[CLS]Marius Constant communicated in[P][T][SEP]\n",
      "[CLS]Nanni Moretti communicated in[P][T][SEP]\n",
      "[CLS]The law in Haninge Municipality defines the[P]language as the official language[T][SEP]\n",
      "[CLS]El Djem is located in[P][T][SEP]\n",
      "[CLS]Nokia E71, produced by[P][T][SEP]\n",
      "[CLS]Auckland Airport, named in[P]'s honor[T][SEP]\n",
      "[CLS]Anna Karenina was a work in the[P]language[T][SEP]\n",
      "[CLS]Manila Accord is located in[P][T][SEP]\n",
      "[CLS]Birmingham pub bombings is located in[P][T][SEP]\n",
      "[CLS]Ferruccio Busoni used to communicate in[P][T][SEP]\n",
      "[CLS]as a citizen of[P], Jan Machulski[T][SEP]\n",
      "[CLS]Michael Winner used the[P]language[T][SEP]\n",
      "[CLS]physical system works in the area of[P][T][SEP]\n",
      "[CLS]In Idaho,[P]is the official language[T][SEP]\n",
      "[CLS]Sergey Lukyanenko worked in[P][T][SEP]\n",
      "[CLS]Blackburn railway station is named in[P]'s honor[T][SEP]\n",
      "[CLS]Alexander Belyavsky died in[P][T][SEP]\n",
      "[CLS]normative ethics is a part of[P][T][SEP]\n",
      "[CLS][P]is Lubaba bint al-Harith's official religion[T][SEP]\n",
      "[CLS]Lev Kamenev communicated in[P][T][SEP]\n",
      "[CLS]The official language of Malawi is the[P]language[T][SEP]\n",
      "[CLS]Christopher Logue used the[P]language[T][SEP]\n",
      "[CLS]The capital city of State of Brazil is[P][T][SEP]\n",
      "[CLS]The Gene Autry Show premiered on[P][T][SEP]\n",
      "[CLS][P]debuted Dateline NBC[T][SEP]\n",
      "[CLS][P], the creator of the Google Desktop[T][SEP]\n",
      "[CLS]Nicolas Gigault was born in[P][T][SEP]\n",
      "[CLS]The native language of Alexandre de Laborde is[P][T][SEP]\n",
      "[CLS]Kay O'Brien premieres on[P][T][SEP]\n",
      "[CLS][P]owns Amazon Music[T][SEP]\n",
      "[CLS][P], the creator of the Chevrolet Corvette C2[T][SEP]\n",
      "[CLS]The capital of Union between Sweden and Norway is[P][T][SEP]\n",
      "[CLS]Georges Couthon passed away in[P][T][SEP]\n",
      "[CLS]Maria Petrovykh used the[P]language[T][SEP]\n",
      "[CLS]Girl Code debuted on[P][T][SEP]\n",
      "[CLS]explicit memory, which is a subclass of[P][T][SEP]\n",
      "[CLS]Vic Sotto originated from[P][T][SEP]\n",
      "[CLS][P]musicians such as Paul Gonsalves[T][SEP]\n",
      "[CLS]BMW M54, developed by[P][T][SEP]\n",
      "[CLS]Degrassi High, that was formulated in[P][T][SEP]\n",
      "[CLS]Santiago Carrillo passed away in[P][T][SEP]\n",
      "[CLS]verbal noun,  a type of[P][T][SEP]\n",
      "[CLS]Unai Emery plays as[P][T][SEP]\n",
      "[CLS]woodburytype specializes in[P][T][SEP]\n",
      "[CLS]Event Viewer was developed by[P][T][SEP]\n",
      "[CLS]Indelibly Stamped was written in[P][T][SEP]\n",
      "[CLS]The headquarter of Aigle Azur is located in[P][T][SEP]\n",
      "[CLS]The Legend of Zelda was developed by[P][T][SEP]\n",
      "[CLS]Adolfo Marsillach communicated in[P][T][SEP]\n",
      "[CLS]patella,  a type of[P][T][SEP]\n",
      "[CLS][P], the creator of the Honda Legend[T][SEP]\n",
      "[CLS]Portuguese cuisine, formulated in[P][T][SEP]\n",
      "[CLS]The capital of Wake County is[P][T][SEP]\n",
      "[CLS][P]debuted The Alcoa Hour[T][SEP]\n",
      "[CLS][P]is Jean Sorel's native language[T][SEP]\n",
      "[CLS]Minneapolis City Hall is located in[P][T][SEP]\n",
      "[CLS]The language of Redfern Now is[P][T][SEP]\n",
      "[CLS][P]owns Google Finance[T][SEP]\n",
      "[CLS]Foulis Castle is located in[P][T][SEP]\n",
      "[CLS]Sergo Kldiashvili used to communicate in[P][T][SEP]\n",
      "[CLS]Gmina Kalinowo is located in[P][T][SEP]\n",
      "[CLS]The original language of Nicholas Nickleby is[P][T][SEP]\n",
      "[CLS]Avram Iancu used the[P]language[T][SEP]\n",
      "[CLS]as a citizen of[P], Shah Shujah Durrani Sadozai[T][SEP]\n",
      "[CLS][P]is the official language of Grand Duchy of Moscow[T][SEP]\n",
      "[CLS]Standard German, which is a subclass of[P][T][SEP]\n",
      "[CLS]Triumph Stag, produced by[P][T][SEP]\n",
      "[CLS]Jean Hyppolite used to communicate in[P][T][SEP]\n",
      "[CLS]as a citizen of[P], Kinji Fukasaku[T][SEP]\n",
      "[CLS]Gazzola is located in[P][T][SEP]\n",
      "[CLS]Fendley Glacier is located in the continent[P][T][SEP]\n",
      "[CLS]Nach Baliye, formulated in[P][T][SEP]\n",
      "[CLS]The[P]language is the official language of Northwest Territories[T][SEP]\n",
      "[CLS]Friedrich Bessel has a citizenship of[P][T][SEP]\n",
      "[CLS]Palazzo Borghese is located in[P][T][SEP]\n",
      "[CLS]Samsung C&T Corporation is a part of[P][T][SEP]\n",
      "[CLS]Tiber is a part of the continent of[P][T][SEP]\n",
      "[CLS]Joseph Goebbels was employed in[P][T][SEP]\n",
      "[CLS]East Jerusalem is part of[P][T][SEP]\n",
      "[CLS]The capital of Grand Duchy of Finland is[P][T][SEP]\n",
      "[CLS]Alex Teixeira, who plays in[P]position[T][SEP]\n",
      "[CLS]Gambacorta Peak belongs to the continent of[P][T][SEP]\n",
      "[CLS]Henry Sargent died at[P][T][SEP]\n",
      "[CLS]Daily Express is written in[P][T][SEP]\n",
      "[CLS]Martin Ericsson plays in the position of[P][T][SEP]\n",
      "[CLS]Georges Ohnet communicated in[P][T][SEP]\n",
      "[CLS]Rodolfo Mederos is a citizen of[P][T][SEP]\n",
      "[CLS]The headquarter of Real Academia de Bellas Artes de San Fernando is located in[P][T][SEP]\n",
      "[CLS]Sextus Pompeius Festus typically used[P]to communicate[T][SEP]\n",
      "[CLS]number theory is part of[P][T][SEP]\n",
      "[CLS]Intercellular adhesion molecule 1, which is a subclass of[P][T][SEP]\n",
      "[CLS]Willamette Valley AVA is in[P][T][SEP]\n",
      "[CLS]Aleksejs Saramotins has a citizenship of[P][T][SEP]\n",
      "[CLS]as a citizen of[P], Rajeev Chandrasekhar[T][SEP]\n",
      "[CLS]Toyota Coaster, created by[P][T][SEP]\n",
      "[CLS]Silverstein Peak is a part of the continent of[P][T][SEP]\n",
      "[CLS]Cape Kinsey is a part of the continent of[P][T][SEP]\n",
      "[CLS]The law in Viitasaari defines[P]as the official language[T][SEP]\n",
      "[CLS]The headquarter of Amirkabir University of Technology is located in[P][T][SEP]\n",
      "[CLS]Seventeen is written in the[P]language[T][SEP]\n",
      "[CLS]Ulrich Wilcken found employment in[P][T][SEP]\n",
      "[CLS]Shaykh Syed Mir Mirak Andrabi is follower of[P][T][SEP]\n",
      "[CLS]Peterborough Cathedral, which is named in[P]'s honor[T][SEP]\n",
      "[CLS]Datsun Sports, a product developed by[P],[T][SEP]\n",
      "[CLS][P], the creator of the Intel 80286[T][SEP]\n",
      "[CLS]Brown University's headquarters are in[P][T][SEP]\n",
      "[CLS]Ferrari 158 is developed by[P][T][SEP]\n",
      "[CLS][P]released The Bob Newhart Show[T][SEP]\n",
      "[CLS]as a citizen of[P], Vesa Vierikko[T][SEP]\n",
      "[CLS]Hammond Civic Center is located in[P][T][SEP]\n",
      "[CLS]John Oldcastle is a citizen of[P][T][SEP]\n",
      "[CLS]ringwork castle, which is a subclass of[P][T][SEP]\n",
      "[CLS]Microsoft HealthVault was developed by[P][T][SEP]\n",
      "[CLS]The[P]-language creation Star Wars sequel trilogy[T][SEP]\n",
      "[CLS]Znamya is written in the[P]language[T][SEP]\n",
      "[CLS]Moldova is located in[P][T][SEP]\n",
      "[CLS]Phaidon Press's headquarters are in[P][T][SEP]\n",
      "[CLS]The mother tongue of Alain Robbe-Grillet is[P][T][SEP]\n",
      "[CLS]Jean Debucourt used the[P]language[T][SEP]\n",
      "[CLS][P]debuted CBS Storybreak[T][SEP]\n",
      "[CLS]Airbus A330 is developed by[P][T][SEP]\n",
      "[CLS]Toyota MR2, produced by[P][T][SEP]\n",
      "[CLS]Harris Peninsula belongs to the continent of[P][T][SEP]\n",
      "[CLS]as a citizen of[P], Duiliu Zamfirescu[T][SEP]\n",
      "[CLS]Veronica Lario has a citizenship of[P][T][SEP]\n",
      "[CLS]Gio Linh, which is located in[P][T][SEP]\n",
      "[CLS]Osman I is affiliated with the[P]religion[T][SEP]\n",
      "[CLS]Altered Beast was created by[P][T][SEP]\n",
      "[CLS]Toronto Star was an[P]-language work[T][SEP]\n",
      "[CLS]Jean-Pierre Dionnet used to communicate in[P][T][SEP]\n",
      "[CLS]Floricienta, that was created in[P][T][SEP]\n",
      "[CLS]Salvatore Accardo used to communicate in[P][T][SEP]\n",
      "[CLS]Toyota Avanza, created by[P][T][SEP]\n",
      "[CLS]Malaysian English is a subclass of[P][T][SEP]\n",
      "[CLS]Jean-Jacques Annaud used the[P]language[T][SEP]\n",
      "[CLS]Officially,[P]is the language of Les Bois[T][SEP]\n",
      "[CLS]MacKenzie Bay belongs to the continent of[P][T][SEP]\n",
      "[CLS][P]is Michel Poniatowski's native language[T][SEP]\n",
      "[CLS]Lennie Tristano originates from[P][T][SEP]\n",
      "[CLS][P]is Bede's official religion[T][SEP]\n",
      "[CLS]University of Southampton is located in[P][T][SEP]\n",
      "[CLS]BBC Radio 1 is owned by[P][T][SEP]\n",
      "[CLS]Nissan Elgrand is developed by[P][T][SEP]\n",
      "[CLS]The law in Iisalmi defines the[P]language as the official language[T][SEP]\n",
      "[CLS]The native language of Anatoly Kudryavitsky is[P][T][SEP]\n",
      "[CLS]Pedro de Heredia used the[P]language[T][SEP]\n",
      "[CLS]as a citizen of[P], Paavo Haavikko[T][SEP]\n",
      "[CLS]Ge You was born in[P][T][SEP]\n",
      "[CLS]IBM AP-101, produced by[P][T][SEP]\n",
      "[CLS]Simon Boccanegra, developed in[P][T][SEP]\n",
      "[CLS]Luke Hemsworth, who has a citizenship of[P][T][SEP]\n",
      "[CLS]Michel Houellebecq holds a citizenship of[P][T][SEP]\n",
      "[CLS]Peter Benchley communicated in[P][T][SEP]\n",
      "[CLS]as a citizen of[P], Robert Persons[T][SEP]\n",
      "[CLS]Porsche 910, developed by[P][T][SEP]\n",
      "[CLS]Toyota Avalon, developed by[P][T][SEP]\n",
      "[CLS]The native language of Leslie Caron is[P][T][SEP]\n",
      "[CLS]My-HiME, that was developed in[P][T][SEP]\n",
      "[CLS]Hussein Chalayan took up work in[P][T][SEP]\n",
      "[CLS]Adolf Brand found employment in[P][T][SEP]\n",
      "[CLS]Ulrike Haage found employment in[P][T][SEP]\n",
      "[CLS]Airbus A320 is developed by[P][T][SEP]\n",
      "[CLS]Fiat 500X is a product of[P][T][SEP]\n",
      "[CLS]as a citizen of[P], Pir Mazhar Ul Haq[T][SEP]\n",
      "[CLS]The capital of Libya is[P][T][SEP]\n",
      "[CLS]Josep Tarradellas i Joan died in[P][T][SEP]\n",
      "[CLS]The headquarter of Italian Democratic Socialist Party is in[P][T][SEP]\n",
      "[CLS]Ada Yonath holds a citizenship of[P][T][SEP]\n",
      "[CLS]Airbus A350 is created by[P][T][SEP]\n",
      "[CLS]Ribeirinho died in[P][T][SEP]\n",
      "[CLS][P]and its product Acura ZDX[T][SEP]\n",
      "[CLS]Miguel A. Torres used to communicate in[P][T][SEP]\n",
      "[CLS]Patricia Wartusch holds a citizenship of[P][T][SEP]\n",
      "[CLS]Wii MotionPlus was developed by[P][T][SEP]\n",
      "[CLS]Mulock Glacier is located in the continent[P][T][SEP]\n",
      "[CLS]The original language of Wah-Wah was[P][T][SEP]\n",
      "[CLS][P]is the official language of Kannonkoski[T][SEP]\n",
      "[CLS]Luigi Boccherini used to communicate in[P][T][SEP]\n",
      "[CLS]Fabio Pecchia plays as[P][T][SEP]\n",
      "[CLS]Yuri Shchekochikhin worked in[P][T][SEP]\n",
      "[CLS]Mookencheril Cherian Joseph is a citizen of[P][T][SEP]\n",
      "[CLS]MS-DOS, a product of[P][T][SEP]\n",
      "[CLS]Elf Aquitaine, formulated in[P][T][SEP]\n",
      "[CLS]Olivier Baroux used the[P]language[T][SEP]\n",
      "[CLS]The original language of Nu ska vi sjunga is[P][T][SEP]\n",
      "[CLS][P]is the language of Finnish Wikipedia[T][SEP]\n",
      "[CLS]NBA on NBC premiered on[P][T][SEP]\n",
      "[CLS]Antonio Soler used the[P]language[T][SEP]\n",
      "[CLS]Hooters's headquarters are in[P][T][SEP]\n",
      "[CLS]Make Them Suffer originated in[P][T][SEP]\n",
      "[CLS]The capital of Indonesia is[P][T][SEP]\n",
      "[CLS]The Beatles was written in the[P]language[T][SEP]\n",
      "[CLS]Chevalier de Saint-George passed away at[P][T][SEP]\n",
      "[CLS]Taniec z gwiazdami, that was developed in[P][T][SEP]\n",
      "[CLS]The[P]-language creation Hava Nagila[T][SEP]\n",
      "[CLS][P]is Maurice Renard's native language[T][SEP]\n",
      "[CLS]BMW X6, a product created by[P],[T][SEP]\n",
      "[CLS]Comoros belongs to the continent of[P][T][SEP]\n",
      "[CLS]as a citizen of[P], Thomas Wentworth, 1st Baron Wentworth[T][SEP]\n",
      "[CLS]Porsche 910 is a product of[P][T][SEP]\n",
      "[CLS]Scott Morrison holds a citizenship of[P][T][SEP]\n",
      "[CLS]Rydberg Peninsula is located in the continent[P][T][SEP]\n",
      "[CLS]John Frederick Maurice was born in[P][T][SEP]\n",
      "[CLS]Lennox Randal Francis Berkeley passed away in[P][T][SEP]\n",
      "[CLS]The Who Sell Out was a work in the[P]language[T][SEP]\n",
      "[CLS]The native language of Virgil is[P][T][SEP]\n",
      "[CLS]Irish Museum of Modern Art is located in[P][T][SEP]\n"
     ]
    }
   ],
   "source": [
    "set_seed(args.seed)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "logger.info('Loading model, tokenizer, etc.')\n",
    "config, model, tokenizer = load_pretrained(args.model_name)\n",
    "model.to(device)\n",
    "embeddings = get_embeddings(model, config)\n",
    "embedding_gradient = GradientStorage(embeddings)\n",
    "predictor = PredictWrapper(model)\n",
    "\n",
    "if args.label_map is not None:\n",
    "    label_map = json.loads(args.label_map)\n",
    "    logger.info(f\"Label map: {label_map}\")\n",
    "else:\n",
    "    label_map = None\n",
    "    logger.info('No label map')\n",
    "templatizer = utils_v4.TriggerTemplatizer(\n",
    "    args.template,\n",
    "    config,\n",
    "    tokenizer,\n",
    "    label_map=label_map,\n",
    "    label_field=args.label_field,\n",
    "    tokenize_labels=args.tokenize_labels,\n",
    "    add_special_tokens=False,\n",
    "    use_ctx=args.use_ctx\n",
    ")\n",
    "# Obtain the initial trigger tokens and label mapping\n",
    "if args.initial_trigger:   \n",
    "    initial_trigger = args.initial_trigger\n",
    "    logger.info(f\"initial trigger {initial_trigger}\")\n",
    "    logger.info(\"init ids\")\n",
    "    init_ids = tokenizer.convert_tokens_to_ids(initial_trigger)\n",
    "    logger.info(init_ids)\n",
    "    init_ids = torch.tensor(init_ids, device=device).unsqueeze(0)\n",
    "    logger.info(init_ids)\n",
    "    trigger_ids = tokenizer.convert_tokens_to_ids(initial_trigger)\n",
    "    logger.info(f'Initial triggers are the following: {initial_trigger}')\n",
    "    logger.info(f'Initial Trigger ids are: {trigger_ids}')\n",
    "    logger.info(f\"len trigger ids: {len(trigger_ids)}\")\n",
    "    logger.info(f\"num trigger tokens: {templatizer.num_trigger_tokens}\")\n",
    "    assert len(trigger_ids) == templatizer.num_trigger_tokens\n",
    "else:\n",
    "    logger.info(f\"no initial trigger provided, using {templatizer.num_trigger_tokens} mask tokens\")\n",
    "    init_ids = [tokenizer.mask_token_id] * templatizer.num_trigger_tokens\n",
    "    init_ids = torch.tensor(init_ids, device=device).unsqueeze(0)\n",
    "    trigger_ids = [tokenizer.mask_token_id] * templatizer.num_trigger_tokens\n",
    "trigger_ids = torch.tensor(trigger_ids, device=device).unsqueeze(0)\n",
    "best_trigger_ids = trigger_ids.clone()\n",
    "# NOTE: Accuracy can only be computed if a fixed pool of labels is given, which currently\n",
    "# requires the label map to be specified. Since producing a label map may be cumbersome (e.g.,\n",
    "# for link prediction tasks), we just use (negative) loss as the evaluation metric in these cases.\n",
    "if label_map:\n",
    "    evaluation_fn = AccuracyFn(tokenizer, label_map, device)\n",
    "else:\n",
    "    evaluation_fn = lambda x, y: -get_loss(x, y)\n",
    "logger.info('Loading datasets')\n",
    "collator = utils_v4.Collator(pad_token_id=tokenizer.pad_token_id)\n",
    "if args.perturbed:\n",
    "    train_dataset = utils_v4.load_augmented_trigger_dataset(args.train, templatizer, limit=args.limit)\n",
    "else:\n",
    "    train_dataset = utils_v4.load_trigger_dataset(args.train, templatizer, start_idx=0, use_ctx=args.use_ctx, limit=args.limit)\n",
    "train_loader = DataLoader(train_dataset, batch_size=args.bsz, shuffle=False, collate_fn=collator)\n",
    "allowed_words = ['iPhone', 'McC', 'YouTube', 'McDonald', 'LinkedIn', 'MPs', 'WhatsApp', 'iOS', 'McCain', 'McG', 'McD', 'McConnell', 'McGregor', 'McCarthy', 'iPad', 'LeBron', 'JPMorgan', 'IoT', 'OnePlus', 'realDonaldTrump', 'BuzzFeed', 'iTunes', 'iPhones', 'SpaceX', 'McLaren', 'PhD', 'PlayStation', 'McKin', 'McCabe', 'McCoy', 'TVs', 'FedEx', 'McGr', 'McGu', 'McMahon', 'CEOs', 'McMaster', 'JavaScript', 'WikiLeaks', 'eBay', 'McKenzie', 'McInt', 'BlackBerry', 'McCorm', 'DeVos', 'PayPal', 'MacBook', 'McCull', 'PCs', 'McKay', 'MacDonald', 'McCann', 'McGee', 'NGOs', 'GHz', 'McKenna', 'McCartney', 'HuffPost', 'McGill', 'WiFi', 'McDonnell', 'iPads', 'GoPro', 'iPod', 'MacArthur', 'VMware', 'macOS', 'CDs', 'McAuliffe', 'WordPress', 'iCloud', 'YouTube', 'GeForce', 'GPUs', 'CPUs', 'GitHub', 'PowerPoint', 'eSports', 'ObamaCare', 'iPhone', 'UFOs', 'mRNA', 'StarCraft', 'LinkedIn']\n",
    "\"\"\"\n",
    "filter = torch.zeros(tokenizer.vocab_size, dtype=torch.float32, device=device)\n",
    "if args.filter:\n",
    "    logger.info('Filtering label tokens.')\n",
    "    if label_map:\n",
    "        for label_tokens in label_map.values():\n",
    "            label_ids = utils.encode_label(tokenizer, label_tokens).unsqueeze(0)\n",
    "            filter[label_ids] = 1e32\n",
    "    else:\n",
    "        for _, label_ids in train_dataset:\n",
    "            filter[label_ids] = 1e32\n",
    "    logger.info('Filtering special tokens and capitalized words.')\n",
    "    for word, idx in tokenizer.get_vocab().items():\n",
    "        if len(word) == 1 or idx >= tokenizer.vocab_size:\n",
    "            continue\n",
    "        # Filter special tokens.\n",
    "        if idx in tokenizer.all_special_ids:\n",
    "            logger.info('Filtered: %s, index: %d', word, idx)\n",
    "            filter[idx] = 1e32\n",
    "        \n",
    "        if isVariable(idx, tokenizer, allowed_words):\n",
    "            logger.debug(f\"Filtered {word}\")\n",
    "            print(word)\n",
    "            filter[idx] = 1e32\n",
    "    \n",
    "        if is_all_capps_or_num(idx, tokenizer):\n",
    "            logger.debug(f\"Filtered {word}\")\n",
    "            print(word)\n",
    "            filter[idx] = 1e32\n",
    "\n",
    "# creating the filter for the first iteration of token generation\n",
    "first_iter_filter = filter.detach().clone()\n",
    "if args.model_name == \"roberta-large\" or args.model_name == 'bert-large-cased':\n",
    "    with open(args.filtered_vocab, \"r\", encoding=\"utf-8\") as f:\n",
    "        whole_word_tokens = json.load(f)\n",
    "    for index in range(tokenizer.vocab_size):\n",
    "        if index not in whole_word_tokens.values():\n",
    "            first_iter_filter[index] = 1e32\n",
    "# end creating first iter filter\n",
    "# Save filter\n",
    "torch.save(first_iter_filter, f\"/home/zsarwar/NLP/autoprompt/data/filters/first_iter_filter_{args.model_name}.pt\")\n",
    "torch.save(filter, f\"/home/zsarwar/NLP/autoprompt/data/filters/filter_{args.model_name}.pt\")\n",
    "\"\"\"\n",
    "first_iter_filter = torch.load(f\"/home/zsarwar/NLP/autoprompt/data/filters/first_iter_filter_{args.model_name}.pt\", map_location=device)\n",
    "filter = torch.load(f\"/home/zsarwar/NLP/autoprompt/data/filters/filter_{args.model_name}.pt\", map_location=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "500it [00:16, 30.51it/s]\n"
     ]
    }
   ],
   "source": [
    "all_model_inputs = []\n",
    "all_labels = []\n",
    "all_pred_labels = []\n",
    "all_indices = []\n",
    "\n",
    "\n",
    "logger.info('Evaluating baseline')\n",
    "logger.info(f\"Baseline trigger ids are : {trigger_ids}\")\n",
    "numerator = 0\n",
    "numerator_acc = 0\n",
    "denominator = 0\n",
    "for idx, (model_inputs, labels) in tqdm(enumerate(train_loader)):\n",
    "    model_inputs = {k: v.to(device) for k, v in model_inputs.items()}\n",
    "    all_indices.append(idx)\n",
    "    labels = labels.to(device)\n",
    "    with torch.no_grad():\n",
    "        predict_logits, m_inputs = predictor(model_inputs, trigger_ids)\n",
    "    pred_label = get_pred_label(predict_logits, labels, tokenizer)\n",
    "    logger.info(f\"Index : {idx}\")\n",
    "    logger.info(f\"Input : {tokenizer.decode(m_inputs['input_ids'][0])}\")\n",
    "    logger.info(f\"Label : {tokenizer.decode(labels[0])}\")\n",
    "    logger.info(f\"Pred : {tokenizer.decode(pred_label[0])}\")\n",
    "    logger.info(f\"\\n\\n\")\n",
    "    all_model_inputs.append(m_inputs)\n",
    "    all_labels.append(labels)\n",
    "    all_pred_labels.append(pred_label)\n",
    "    numerator += evaluation_fn(predict_logits, labels).sum().item()\n",
    "    denominator += labels.size(0)\n",
    "    numerator_acc += compute_accuracy(predict_logits, labels)\n",
    "dev_metric = numerator / (denominator + 1e-13)\n",
    "acc_metric_base = numerator_acc / (denominator + 1e-13)\n",
    "logger.info(f'Dev metric: {dev_metric}')\n",
    "logger.info(f'Dev acc metric baseline is : {acc_metric_base}')\n",
    "best_dev_metric = 10\n",
    "best_dev_acc_metric = 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# precalculating the normalized embeddings\n",
    "embed_norm = torch.linalg.vector_norm(embeddings.weight, dim=1)\n",
    "normalized_embedding_weights = torch.transpose(\n",
    "    torch.divide(torch.transpose(embeddings.weight, 0, 1), embed_norm),\n",
    "    0,\n",
    "    1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.include_gpt:\n",
    "    # intializing GPT-2\n",
    "    gpt_model = GPT2LMHeadModel.from_pretrained('gpt2-xl')\n",
    "    gpt_tokenizer = GPT2Tokenizer.from_pretrained('gpt2-xl')\n",
    "    gpt_tokenizer.pad_token_id = gpt_tokenizer.eos_token_id\n",
    "    gpt_tokenizer.padding_side = \"left\"\n",
    "    gpt_model = gpt_model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "1it [00:05,  5.72s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "2it [00:11,  5.81s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "3it [00:16,  5.62s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "4it [00:22,  5.63s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "5it [00:27,  5.47s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "6it [00:33,  5.41s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "7it [00:38,  5.42s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "8it [00:43,  5.37s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "9it [00:49,  5.39s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "10it [00:54,  5.47s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "11it [01:00,  5.54s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "12it [01:06,  5.57s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "13it [01:11,  5.51s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "14it [01:16,  5.46s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "15it [01:22,  5.52s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "16it [01:28,  5.49s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "17it [01:33,  5.49s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "18it [01:38,  5.41s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "19it [01:44,  5.39s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "20it [01:49,  5.40s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "21it [01:55,  5.45s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "22it [02:00,  5.45s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "23it [02:06,  5.48s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "24it [02:11,  5.40s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "25it [02:16,  5.46s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "26it [02:22,  5.43s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "27it [02:27,  5.45s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "28it [02:33,  5.55s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "29it [02:39,  5.54s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "30it [02:44,  5.46s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "31it [02:49,  5.50s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "32it [02:55,  5.54s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "33it [03:00,  5.51s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "34it [03:06,  5.43s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "35it [03:11,  5.46s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "36it [03:17,  5.48s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "37it [03:22,  5.53s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "38it [03:28,  5.58s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "39it [03:34,  5.55s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "40it [03:39,  5.57s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "41it [03:45,  5.50s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "42it [03:50,  5.52s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "43it [03:56,  5.50s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "44it [04:01,  5.49s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "45it [04:06,  5.46s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "46it [04:12,  5.48s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "47it [04:17,  5.49s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "48it [04:23,  5.51s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "49it [04:29,  5.60s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "50it [04:34,  5.50s/it]\n"
     ]
    }
   ],
   "source": [
    "all_model_inputs_triggers = []\n",
    "all_labels_triggers = []\n",
    "all_pred_labels_triggers = []\n",
    "all_gpt_encodings = []\n",
    "all_gpt_generations = []\n",
    "all_model_encodings = []\n",
    "all_adv_tokens = []\n",
    "all_indices_triggers = []\n",
    "all_sub_indices_triggers = []\n",
    "\n",
    "\n",
    "new_example = True\n",
    "total_samples = 0\n",
    "total_incorrect = 0\n",
    "model.zero_grad()\n",
    "averaged_grad = None\n",
    "# Accumulate\n",
    "for idx, (model_inputs, labels) in tqdm(enumerate(train_loader)):\n",
    "    new_example=True\n",
    "    total_samples+=1    \n",
    "    if(total_samples > 50) :\n",
    "        break\n",
    "    # Start from scratch for each example\n",
    "    all_indices_triggers.append(idx)\n",
    "    trigger_ids = init_ids.clone()\n",
    "    model.zero_grad()\n",
    "    model_inputs = {k: v.to(device) for k, v in model_inputs.items()}\n",
    "    labels = labels.to(device)\n",
    "    with torch.no_grad():   \n",
    "        predict_logits, _ = predictor(model_inputs, trigger_ids)\n",
    "        eval_metric = evaluation_fn(predict_logits, labels)\n",
    "        eval_acc_metric = compute_accuracy(predict_logits, labels)\n",
    "    for token_to_flip in range(templatizer.num_trigger_tokens):\n",
    "        model.zero_grad()\n",
    "        predict_logits, _ = predictor(model_inputs, trigger_ids)\n",
    "        loss = get_loss(predict_logits, labels).mean()\n",
    "        loss.backward()\n",
    "        grad = embedding_gradient.get()\n",
    "        bsz, _, emb_dim = grad.size()\n",
    "        selection_mask = model_inputs['trigger_mask'].unsqueeze(-1)\n",
    "        grad = torch.masked_select(grad, selection_mask)\n",
    "        grad = grad.view(bsz, templatizer.num_trigger_tokens, emb_dim)\n",
    "        averaged_grad = grad.sum(dim=0)\n",
    "        all_labels_triggers.append(labels)\n",
    "\n",
    "        # Compute adv tokens in any case    \n",
    "        candidates = hotflip_attack(averaged_grad[token_to_flip],\n",
    "                                    normalized_embedding_weights,\n",
    "                                    increase_loss=True,\n",
    "                                    num_candidates=args.num_cand,\n",
    "                                    filter=filter if token_to_flip > 0 else first_iter_filter)\n",
    "            \n",
    "        all_adv_tokens.append(candidates)\n",
    "        current_score = 0\n",
    "        current_acc = 0\n",
    "        candidate_scores = torch.zeros(args.num_cand, device=device)\n",
    "        candidate_accs = torch.zeros(args.num_cand, device=device)\n",
    "        candidate_pred_labels = torch.zeros(args.num_cand, device=device, dtype=int)\n",
    "        denom = 0\n",
    "        all_candidates = []\n",
    "        entire_text = []\n",
    "        # Update current score\n",
    "        current_acc = eval_acc_metric\n",
    "        current_score = eval_metric.sum()\n",
    "        denom = labels.size(0)    \n",
    "        # Changes start from here\n",
    "        # Batched og prompts in id form\n",
    "        original_prompt_ids = model_inputs['input_ids'][0].unsqueeze(0)\n",
    "        original_prompt_ids = original_prompt_ids.repeat(args.num_cand, 1)\n",
    "        rep_token_idx = torch.where(original_prompt_ids == tokenizer.mask_token_id)[1][0]\n",
    "        original_prompt_ids[:,rep_token_idx] = labels[0].item()\n",
    "        # Batched trigger prompts in id form\n",
    "        temp_trigger = trigger_ids.clone()\n",
    "        temp_triggers = temp_trigger.repeat(len(candidates), 1)\n",
    "        temp_triggers[:, token_to_flip] = candidates\n",
    "        # Batched og + trigger prompts in text form\n",
    "        original_prompts = tokenizer.batch_decode(original_prompt_ids, skip_special_tokens=True)\n",
    "        candidates_strs = tokenizer.batch_decode(candidates.unsqueeze(1))\n",
    "        \n",
    "        if(args.include_adv_token):\n",
    "            if(\"roberta\" in args.model_name):\n",
    "                pre_text = [original_prompts[i] + candidates_strs[i] for i in range(len(original_prompts))]\n",
    "            elif(\"bert\" in args.model_name):\n",
    "                pre_text = [original_prompts[i] + \" \" + candidates_strs[i] for i in range(len(original_prompts))]    \n",
    "        else:\n",
    "            pre_text = [original_prompts[i] for i in range(len(original_prompts))]\n",
    "\n",
    "        skip_indices = []\n",
    "        if(args.include_gpt):\n",
    "            # Encode for GPT-2 Generations and generate\n",
    "            gpt_encoded_prompts = gpt_tokenizer.batch_encode_plus(pre_text, add_special_tokens=True, return_attention_mask=True, padding='longest', return_tensors='pt').to(device) \n",
    "            all_gpt_encodings.append(gpt_encoded_prompts)\n",
    "            with torch.no_grad():\n",
    "                gpt_outputs = gpt_model.generate(inputs=gpt_encoded_prompts['input_ids'], attention_mask=gpt_encoded_prompts['attention_mask'], do_sample=True, top_p=0.96, output_scores=False, return_dict_in_generate=True, max_length=100)\n",
    "            num_tokens = gpt_encoded_prompts['input_ids'][0].numel()\n",
    "            # Need Entire GPT-2 Text here for entire text\n",
    "            gpt_all_tokens = gpt_outputs['sequences']\n",
    "            all_gpt_generations.append(gpt_all_tokens)\n",
    "            gpt_all_tokens_str = gpt_tokenizer.batch_decode(gpt_all_tokens, skip_special_tokens=True)\n",
    "            #NLTK for all_tokens    \n",
    "            gpt_all_str_sents = [tokenize.sent_tokenize(sent) for sent in gpt_all_tokens_str]\n",
    "            \n",
    "            if(args.remove_periods):\n",
    "                gpt_gen_with_og = [all_sents[0] for all_sents in gpt_all_str_sents]\n",
    "            else:\n",
    "                gpt_gen_with_og = [' '.join(all_sents[0:2]) for all_sents in gpt_all_str_sents]             \n",
    "            entire_text = gpt_gen_with_og\n",
    "            # Separate the newly generated tokens\n",
    "            gpt_new_tokens = gpt_outputs['sequences'][:, num_tokens:]\n",
    "            gpt_new_tokens_str = gpt_tokenizer.batch_decode(gpt_new_tokens, skip_special_tokens=True)\n",
    "            # NLTK for new_tokens\n",
    "            gpt_new_str_sents = [tokenize.sent_tokenize(sent) for sent in gpt_new_tokens_str]\n",
    "            gpt_gen = [all_sents[0] if(len(all_sents) >= 1 ) else \"SKIPPING\" for all_sents in gpt_new_str_sents]\n",
    "            skip_indices = [i for i, sent in enumerate(gpt_gen) if sent == \"SKIPPING\"]\n",
    "            # Maybe add length check?\n",
    "        \n",
    "        elif(args.include_adv_token):\n",
    "            skip_indices = []\n",
    "            entire_text = pre_text\n",
    "\n",
    "        #TODO Add condition for wikipedia\n",
    "          \n",
    "        \n",
    "        \n",
    "        \n",
    "        # Retokenize trigger tokens into bert with adv. tokens or gpt_generations or wikipedia text or any combination of them.\n",
    "        # Everything needs to be in text here    \n",
    "        # Insert trigger token in the beginning\n",
    "        if(args.include_adv_token):\n",
    "            if(args.include_gpt):\n",
    "                if(\"roberta\" in args.model_name):\n",
    "                    final_trigg_text = [candidates_strs[i] + gpt_gen[i] for i in range(len(gpt_gen))]\n",
    "                elif(\"bert\" in args.model_name):\n",
    "                    final_trigg_text = [\" \" + candidates_strs[i] + gpt_gen[i] for i in range(len(gpt_gen))]\n",
    "            else:\n",
    "                if(\"roberta\" in args.model_name):\n",
    "                    final_trigg_text = candidates_strs\n",
    "                elif(\"bert\" in args.model_name):\n",
    "                    final_trigg_text = [\" \" + candidates_strs[i] for i in range(len(candidates_strs))]\n",
    "        elif(args.include_gpt):\n",
    "            if(\"roberta\" in args.model_name):\n",
    "                final_trigg_text = gpt_gen\n",
    "            elif(\"bert\" in args.model_name):\n",
    "                final_trigg_text = [\" \" + gpt_gen[i] for i in range(len(gpt_gen))]\n",
    "        # Tokenize trigger text into to-be-attacked models token ids\n",
    "        final_trigg_tokens = tokenizer.batch_encode_plus(final_trigg_text, add_special_tokens=False)\n",
    "        all_candidates = final_trigg_tokens['input_ids']\n",
    "        # Evaluate with adversarial prompts\n",
    "        curr_inputs = []\n",
    "        curr_pred_labels = []\n",
    "\n",
    "        sub_indices = []\n",
    "\n",
    "        for j in range(len(all_candidates)):\n",
    "            \n",
    "            if j not in skip_indices:\n",
    "                trigg_toks = torch.tensor(all_candidates[j], device=device).unsqueeze(0)\n",
    "                with torch.no_grad():\n",
    "                    predict_logits, m_inpts = predictor(model_inputs, trigg_toks)\n",
    "                    eval_metric = evaluation_fn(predict_logits, labels)\n",
    "                    pred_label = get_pred_label(predict_logits, labels, tokenizer)\n",
    "                    eval_attack_acc_metric = compute_accuracy(predict_logits, labels)\n",
    "                curr_inputs.append(m_inpts)\n",
    "                curr_pred_labels.append(pred_label)\n",
    "                candidate_scores[j] = eval_metric.sum()\n",
    "                candidate_accs[j] = eval_attack_acc_metric\n",
    "                candidate_pred_labels[j] = pred_label\n",
    "            else:\n",
    "                logger.info(\"Skipping because of empty generation sequence\")\n",
    "                curr_inputs.append(-100)\n",
    "                curr_pred_labels.append(-100)\n",
    "        \n",
    "        all_model_inputs_triggers.append(curr_inputs)\n",
    "        all_pred_labels_triggers.append(curr_pred_labels)\n",
    "        \n",
    "        # Print and save successful prompts\n",
    "        if(candidate_accs == 0).any():\n",
    "            total_incorrect+=1\n",
    "            logger.info(f\"Index  : {idx}\")\n",
    "            logger.info(f\"Original  : {original_prompts[0]}\")\n",
    "            real_label = tokenizer.convert_ids_to_tokens(labels)\n",
    "            for index, candidate_acc in enumerate(candidate_accs):\n",
    "                sub_indices.append(index)\n",
    "                if index not in skip_indices:\n",
    "                    if candidate_acc != 0:\n",
    "                        continue\n",
    "                    adv_lab = candidate_pred_labels[index].item()\n",
    "                    # Replace only the first instance of the true label with the predicted (adversarial) label\n",
    "                    \n",
    "                    if(\"roberta\" in args.model_name):\n",
    "                        adv_text_pred = entire_text[index].replace(tokenizer.convert_tokens_to_string(real_label[0]), tokenizer.decode(adv_lab), 1)\n",
    "                    elif(\"bert\" in args.model_name):\n",
    "                        adv_text_pred = entire_text[index].replace(tokenizer.convert_tokens_to_string(real_label), tokenizer.decode(adv_lab), 1)\n",
    "                    trigger_ids = all_candidates[index]\n",
    "                    \n",
    "                    logger.info(f\"Adversarial {index}: {adv_text_pred}\")\n",
    "                    \n",
    "            logger.info(f\"\\n\\n\")\n",
    "            all_sub_indices_triggers.append(sub_indices)\n",
    "            break\n",
    "        break\n",
    "flip_rate = total_incorrect / total_samples + 1e-32\n",
    "logger.info(f\"Total incorrect are : {total_incorrect}\")\n",
    "logger.info(f\"Total samples are : {total_samples}\")\n",
    "logger.info(f\"Flip rate is : {flip_rate}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving results\n",
    "all_results_dict = {}\n",
    "\n",
    "results_baseline = {}\n",
    "results_baseline['all_model_inputs'] = all_model_inputs\n",
    "results_baseline['all_labels'] = all_labels\n",
    "results_baseline['all_pred_labels'] = all_pred_labels\n",
    "results_baseline['all_indices'] = all_indices\n",
    "\n",
    "\n",
    "all_results_dict['results_baseline'] = results_baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_adversarial = {}\n",
    "\n",
    "results_adversarial['all_model_inputs_triggers'] = all_model_inputs_triggers\n",
    "results_adversarial['all_labels_triggers'] = all_labels_triggers\n",
    "results_adversarial['all_pred_labels_triggers'] = all_pred_labels_triggers\n",
    "results_adversarial['all_gpt_encodings'] = all_gpt_encodings\n",
    "results_adversarial['all_gpt_generations'] = all_gpt_generations\n",
    "results_adversarial['all_model_encodings'] = all_model_encodings\n",
    "results_adversarial['all_adv_tokens'] = all_adv_tokens\n",
    "results_adversarial['all_indices_triggers'] = all_indices_triggers\n",
    "results_adversarial['all_sub_indices_triggers'] = all_sub_indices_triggers\n",
    "\n",
    "all_results_dict['results_adversarial'] = results_adversarial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] as a citizen of [MASK], Philippe Leclerc de Hauteclocque Okayama was arrested in 2007 in the Belgian city of Bruges after allegedly planning to travel to a Muslim country in order to join jihad. [SEP]'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(all_results_dict['results_adversarial'][\"all_model_inputs_triggers\"][4][5]['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(numpy_file, all_results_dict, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_dict = np.load(numpy_file, allow_pickle=True)\n",
    "loaded_dict = loaded_dict.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] Antonis Samaras has a citizenship of [MASK] Syrian and a residence in the country since he was 18 years old. [SEP]'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(loaded_dict['results_adversarial']['all_model_inputs_triggers'][21][0]['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'today'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(loaded_dict['results_adversarial']['all_pred_labels_triggers'][47][2][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|endoftext|>Antonis Samaras has a citizenship of Greece Argentine.\\n\\n\\nIn 2008, he joined the Argentine Football Association in Argentina and was elected as the president of the federation in 2012 and 2017.\\n\\n\\nHe was named by the Argentine Football Confederation as the most influential people of the past ten years and received the \"La Ficha\" in 2014.\\n\\n\\nHis name appears with that of his father on the Fifa ranking of the most influential people on the world football in 2009 and 2009 again.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt_tokenizer.decode(loaded_dict['results_adversarial']['all_gpt_generations'][21][5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('nlp2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "37239dad5b8b56b0d6556e681c63cac8a09607160e7510acb3444e24aad22957"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
