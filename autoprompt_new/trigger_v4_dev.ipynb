{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TRANSFORMERS_CACHE=/bigstor/zsarwar/models/cache\n",
      "env: CUDA_VISIBLE_DEVICES=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zsarwar/.conda/envs/nlp2/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%set_env TRANSFORMERS_CACHE=/bigstor/zsarwar/models/cache\n",
    "%set_env CUDA_VISIBLE_DEVICES=0\n",
    "import time\n",
    "import argparse\n",
    "import json\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import transformers\n",
    "from transformers import AutoConfig, AutoTokenizer, AutoModelWithLMHead, AutoModelForMaskedLM\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from tqdm import tqdm\n",
    "from nltk import tokenize\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "import nltk \n",
    "import utils_v4\n",
    "from transformers import set_seed as ss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--label-map', type=str, default=None, help='JSON object defining label map')\n",
    "# LAMA-specific\n",
    "parser.add_argument('--tokenize-labels', action='store_true',\n",
    "                help='If specified labels are split into word pieces.'\n",
    "                        'Needed for LAMA probe experiments.')\n",
    "parser.add_argument('--filter', action='store_true', default=True,\n",
    "                help='If specified, filter out special tokens and gold objects.'\n",
    "                        'Furthermore, tokens starting with capital '\n",
    "                        'letters will not appear in triggers. Lazy '\n",
    "                        'approach for removing proper nouns.')\n",
    "parser.add_argument('--print-lama', action='store_true',\n",
    "                help='Prints best trigger in LAMA format.')\n",
    "parser.add_argument('--logfile', type=str, default='debug_jupyter')\n",
    "parser.add_argument('--initial-trigger', nargs='+', type=str, default=None, help='Manual prompt')\n",
    "parser.add_argument('--label-field', type=str, default='Prediction',\n",
    "                help='Name of the label field')\n",
    "parser.add_argument('--bsz', type=int, default=1, help='Batch size')\n",
    "parser.add_argument('--eval-size', type=int, default=1, help='Eval size')\n",
    "parser.add_argument('--iters', type=int, default=1,\n",
    "                help='Number of iterations to run trigger search algorithm')\n",
    "parser.add_argument('--accumulation-steps', type=int, default=1)\n",
    "parser.add_argument('--seed', type=int, default=0)\n",
    "parser.add_argument('--limit', type=int, default=None)\n",
    "parser.add_argument('--use-ctx', action='store_true',\n",
    "                help='Use context sentences for relation extraction only')\n",
    "parser.add_argument('--perturbed', action='store_true',\n",
    "                help='Perturbed sentence evaluation of relation extraction: replace each object in dataset with a random other object')\n",
    "parser.add_argument('--patience', type=int, default=5)\n",
    "\n",
    "parser.add_argument('--sentence-size', type=int, default=50)\n",
    "parser.add_argument('--debug', action='store_true')\n",
    "\n",
    "# Arguments needed in bashfile\n",
    "parser.add_argument('--train', type=Path)\n",
    "parser.add_argument('--template', type=str,default='<s>{Pre_Mask}[Predict_Token]{Post_Mask}[Trigger_Token]</s>', help='Template string', required=False)\n",
    "parser.add_argument('--filtered_vocab', type=str, default=None, help='JSON object defining label map')\n",
    "parser.add_argument('--model-name', type=str, default='bert-large-cased')\n",
    "parser.add_argument('--include_gpt', action='store_true', default=True)\n",
    "parser.add_argument('--include_adv_token', action='store_true', default=False )\n",
    "parser.add_argument('--include_wikipedia_padding', action='store_true', default=False )\n",
    "parser.add_argument('--remove_periods', default=False, action='store_true')\n",
    "parser.add_argument('--num-cand', type=int, default=10)\n",
    "parser.add_argument('--start_idx', type=int, default=0)\n",
    "parser.add_argument('--end_idx', type=int, default=500)\n",
    "parser.add_argument('--tot_gpt_attempts', type=int, default=10)\n",
    "parser.add_argument('--replace_period_with_comma', action='store_true', default=False)\n",
    "args = parser.parse_args([])\n",
    "if args.debug:\n",
    "        level = logging.DEBUG\n",
    "else:\n",
    "        level = logging.INFO\n",
    "if 'roberta' in args.model_name:\n",
    "    args.template = \"<s>{Pre_Mask}[Predict_Token]{Post_Mask}[Trigger_Token]</s>\"\n",
    "    args.train = Path(\"/home/zsarwar/NLP/autoprompt/data/datasets/final/roberta_large_single_entity_2500.jsonl\")\n",
    "elif 'bert' in args.model_name:\n",
    "    args.template = \"[CLS]{Pre_Mask}[Predict_Token]{Post_Mask}[Trigger_Token][SEP]\"\n",
    "    args.train = Path(\"/home/zsarwar/NLP/autoprompt/data/datasets/final/bert_large_cased_2500.jsonl\")\n",
    "logfile = \"/home/zsarwar/NLP/autoprompt/autoprompt/Results/\"+ str(args.train).split(\"/\")[-1].split(\".\")[0]  +  \"_\" + args.logfile    # Change to your own directory\n",
    "numpy_file = \"/home/zsarwar/NLP/autoprompt/autoprompt/Results/Arrays/\" + str(args.train).split(\"/\")[-1].split(\".\")[0]  +  \"_\" + args.logfile + \".npy\" # Change to your own directory\n",
    "logging.basicConfig(filename=logfile,level=level)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "NER = spacy.load(\"en_core_web_sm\")\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.getLogger(\"transformers.tokenization_utils\").setLevel(logging.ERROR)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class GradientStorage:\n",
    "    \"\"\"\n",
    "    This object stores the intermediate gradients of the output a the given PyTorch module, which\n",
    "    otherwise might not be retained.\n",
    "    \"\"\"\n",
    "    def __init__(self, module):\n",
    "        self._stored_gradient = None\n",
    "        module.register_full_backward_hook(self.hook)\n",
    "        # module.register_backward_hook(self.hook)\n",
    "\n",
    "    def hook(self, module, grad_in, grad_out):\n",
    "        self._stored_gradient = grad_out[0]\n",
    "\n",
    "    def get(self):\n",
    "        return self._stored_gradient\n",
    "\n",
    "class PredictWrapper:\n",
    "    \"\"\"\n",
    "    PyTorch transformers model wrapper. Handles necc. preprocessing of inputs for triggers\n",
    "    experiments.\n",
    "    \"\"\"\n",
    "    def __init__(self, model):\n",
    "        self._model = model\n",
    "\n",
    "    def __call__(self, model_inputs, trigger_ids):\n",
    "        model_inputs = model_inputs.copy()\n",
    "        trigger_mask = model_inputs.pop('trigger_mask')\n",
    "        model_inputs = replace_trigger_tokens(model_inputs, trigger_ids, trigger_mask)\n",
    "        predict_mask = model_inputs.pop('predict_mask')\n",
    "        logits = self._model(**model_inputs).logits\n",
    "        predict_logits = logits.masked_select(predict_mask.unsqueeze(-1)).view(logits.size(0), -1)\n",
    "        return predict_logits, model_inputs\n",
    "\n",
    "class AccuracyFn:\n",
    "    \"\"\"\n",
    "    Computing the accuracy when a label is mapped to multiple tokens is difficult in the current\n",
    "    framework, since the data generator only gives us the token ids. To get around this we\n",
    "    compare the target logp to the logp of all labels. If target logp is greater than all (but)\n",
    "    one of the label logps we know we are accurate.\n",
    "    \"\"\"\n",
    "    def __init__(self, tokenizer, label_map, device, tokenize_labels=False):\n",
    "        self._all_label_ids = []\n",
    "        self._pred_to_label = []\n",
    "        logger.info(label_map)\n",
    "        for label, label_tokens in label_map.items():\n",
    "            self._all_label_ids.append(utils.encode_label(tokenizer, label_tokens, tokenize_labels).to(device))\n",
    "            self._pred_to_label.append(label)\n",
    "        logger.info(self._all_label_ids)\n",
    "\n",
    "    def __call__(self, predict_logits, gold_label_ids):\n",
    "        # Get total log-probability for the true label\n",
    "        gold_logp = get_loss(predict_logits, gold_label_ids)\n",
    "\n",
    "        # Get total log-probability for all labels\n",
    "        bsz = predict_logits.size(0)\n",
    "        all_label_logp = []\n",
    "        for label_ids in self._all_label_ids:\n",
    "            label_logp = get_loss(predict_logits, label_ids.repeat(bsz, 1))\n",
    "            all_label_logp.append(label_logp)\n",
    "        all_label_logp = torch.stack(all_label_logp, dim=-1)\n",
    "        _, predictions = all_label_logp.max(dim=-1)\n",
    "        predictions = [self._pred_to_label[x] for x in predictions.tolist()]\n",
    "\n",
    "        # Add up the number of entries where loss is greater than or equal to gold_logp.\n",
    "        ge_count = all_label_logp.le(gold_logp.unsqueeze(-1)).sum(-1)\n",
    "        correct = ge_count.le(1)  # less than in case of num. prec. issues\n",
    "\n",
    "        return correct.float()\n",
    "\n",
    "    # TODO: @rloganiv - This is hacky. Replace with something sensible.\n",
    "    def predict(self, predict_logits):\n",
    "        bsz = predict_logits.size(0)\n",
    "        all_label_logp = []\n",
    "        for label_ids in self._all_label_ids:\n",
    "            label_logp = get_loss(predict_logits, label_ids.repeat(bsz, 1))\n",
    "            all_label_logp.append(label_logp)\n",
    "        all_label_logp = torch.stack(all_label_logp, dim=-1)\n",
    "        _, predictions = all_label_logp.max(dim=-1)\n",
    "        predictions = [self._pred_to_label[x] for x in predictions.tolist()]\n",
    "        return predictions\n",
    "\n",
    "\n",
    "def load_pretrained(model_name):\n",
    "    \"\"\"\n",
    "    Loads pretrained HuggingFace config/model/tokenizer, as well as performs required\n",
    "    initialization steps to facilitate working with triggers.\n",
    "    \"\"\"\n",
    "    config = AutoConfig.from_pretrained(model_name)\n",
    "    model = AutoModelForMaskedLM.from_pretrained(model_name)\n",
    "    model.eval()\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, add_prefix_space=True)\n",
    "    utils_v4.add_task_specific_tokens(tokenizer)\n",
    "    return config, model, tokenizer\n",
    "\n",
    "def set_seed(seed: int):\n",
    "    \"\"\"Sets the relevant random seeds.\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.random.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    ss(seed)\n",
    "\n",
    "def get_embeddings(model, config):\n",
    "    \"\"\"\n",
    "    Returns the wordpiece embedding module.\n",
    "    \"\"\"\n",
    "    base_model = getattr(model, config.model_type)\n",
    "    embeddings = base_model.embeddings.word_embeddings\n",
    "    return embeddings\n",
    "\n",
    "def compute_accuracy(predict_logits, labels):\n",
    "    target_logp = F.log_softmax(predict_logits, dim=-1)\n",
    "    max_pred = torch.argmax(target_logp, dim=-1).unsqueeze(-1)\n",
    "    mask = max_pred.eq(labels)\n",
    "    correct = mask.nonzero().shape[0]\n",
    "    total = labels.shape[0]\n",
    "    acc = correct / total\n",
    "    return correct\n",
    "\n",
    "def hotflip_attack(averaged_grad,\n",
    "                   normalized_embedding_matrix,\n",
    "                   increase_loss=False,\n",
    "                   num_candidates=1,\n",
    "                   filter=None):\n",
    "    \"\"\"Returns the top candidate replacements.\"\"\"\n",
    "    with torch.no_grad():\n",
    "        gradient_dot_embedding_matrix = torch.matmul(\n",
    "            normalized_embedding_matrix,\n",
    "            averaged_grad\n",
    "        )\n",
    "\n",
    "        if filter is not None:\n",
    "            gradient_dot_embedding_matrix -= filter\n",
    "            \n",
    "        if not increase_loss:\n",
    "            gradient_dot_embedding_matrix *= -1\n",
    "\n",
    "    _, top_k_ids = gradient_dot_embedding_matrix.topk(num_candidates)\n",
    "    return top_k_ids\n",
    "\n",
    "def get_pred_label(predict_logits, labels, tokenizer):\n",
    "    target_logp = F.log_softmax(predict_logits, dim=-1)\n",
    "    max_pred = torch.argmax(target_logp, dim=-1).unsqueeze(-1)\n",
    "    return max_pred\n",
    "\n",
    "def get_loss(predict_logits, label_ids):\n",
    "    predict_logp = F.log_softmax(predict_logits, dim=-1)\n",
    "    target_logp = predict_logp.gather(-1, label_ids)\n",
    "    target_logp = target_logp - 1e32 * label_ids.eq(0)  # Apply mask\n",
    "    target_logp = torch.logsumexp(target_logp, dim=-1)\n",
    "    return -target_logp\n",
    "\n",
    "def isVariable(idx, tokenizer, allowed_words):\n",
    "    word = tokenizer.decode([idx])\n",
    "    word = word.replace(\" \", \"\")\n",
    "    _isVar = False\n",
    "    upper_locs = [i for i, ch in enumerate(word) if ch.isupper()]\n",
    "    # Check if caps in between and entire word is not upper-case\n",
    "    if(len(upper_locs) > 0 and len(upper_locs) < len(word)):\n",
    "        for idx in upper_locs:\n",
    "            if (idx > 0):\n",
    "            # Check if token is not real entity like McDonalds                \n",
    "                parsed_word= NER(word)\n",
    "                if (len(parsed_word.ents) == 0):\n",
    "                    if(word not in allowed_words):\n",
    "                        _isVar = True\n",
    "                    break \n",
    "    return _isVar\n",
    "\n",
    "def is_all_capps_or_num(idx, tokenizer):\n",
    "    word = tokenizer.decode([idx])\n",
    "    word = word.replace(\" \", \"\")\n",
    "    _is_all_caps_nums = False\n",
    "    word_upper = word.upper()\n",
    "    if(word_upper == word):\n",
    "        _is_all_caps_nums = True\n",
    "    # Check if it contains a number    \n",
    "    if (any(char.isdigit() for char in word)):\n",
    "        _is_all_caps_nums = True\n",
    "    return _is_all_caps_nums\n",
    "\n",
    "\n",
    "def replace_trigger_tokens(model_inputs, trigger_ids, trigger_mask):\n",
    "    out = model_inputs.copy()    \n",
    "    # Count number of false values\n",
    "    new_len = (torch.count_nonzero(trigger_mask.eq(False)) + trigger_ids.shape[1]).item()\n",
    "    # New trigger mask\n",
    "    new_trigger_mask = torch.zeros(new_len, dtype=torch.bool, device=device).unsqueeze(0)\n",
    "    # Get index of first true element in the old mask and fill in new_trigger_mask\n",
    "    trigger_start_index = torch.where(trigger_mask == True)[1][0].item()\n",
    "    new_trigger_mask[0][trigger_start_index: trigger_start_index + trigger_ids.shape[1]] = True\n",
    "    # New input_ids_tensor\n",
    "    new_input_ids = torch.full(new_trigger_mask.shape, fill_value=-1, device=device)\n",
    "    # Fill in og ids\n",
    "    og_text_ids = (torch.masked_select(out['input_ids'], trigger_mask.eq(False)))\n",
    "    new_input_ids.masked_scatter_(new_trigger_mask.eq(False), og_text_ids)\n",
    "    # Fill in new trigger_ids\n",
    "    new_input_ids.masked_scatter_(new_trigger_mask, trigger_ids)\n",
    "    # New prediction mask\n",
    "    new_pred_mask = torch.full(new_trigger_mask.shape, fill_value=0, device=device,dtype=torch.bool)\n",
    "    # Need to check for number of trigger tokens in both masks\n",
    "    if(\"token_type_ids\" in out):\n",
    "        new_tok_type_ids = torch.zeros(new_trigger_mask.shape, device=device, dtype=torch.int32)\n",
    "        out['token_type_ids'] = new_tok_type_ids\n",
    "    pred_mask_true_index = torch.where(out['predict_mask'])[1][0].item()\n",
    "    num_trig_tokens_old = torch.count_nonzero(trigger_mask)\n",
    "    num_trig_tokens_new = torch.count_nonzero(new_trigger_mask)\n",
    "    diff = num_trig_tokens_new - num_trig_tokens_old\n",
    "    if(trigger_start_index > pred_mask_true_index):\n",
    "        # Copy/paste into the same index as is\n",
    "        new_pred_mask[0][pred_mask_true_index] = True\n",
    "    else:\n",
    "        new_pred_mask[0][pred_mask_true_index + diff] = True\n",
    "    # Finally, a new attention mask is also needed\n",
    "    new_attention_mask = torch.full(new_input_ids.shape, fill_value=1, device=device)\n",
    "    out['input_ids'] = new_input_ids\n",
    "    out['predict_mask'] = new_pred_mask\n",
    "    out['attention_mask'] = new_attention_mask    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "500it [00:14, 35.05it/s]\n"
     ]
    }
   ],
   "source": [
    "set_seed(args.seed)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "logger.info('Loading model, tokenizer, etc.')\n",
    "config, model, tokenizer = load_pretrained(args.model_name)\n",
    "model.to(device)\n",
    "embeddings = get_embeddings(model, config)\n",
    "embedding_gradient = GradientStorage(embeddings)\n",
    "predictor = PredictWrapper(model)\n",
    "\n",
    "if args.label_map is not None:\n",
    "    label_map = json.loads(args.label_map)\n",
    "    logger.info(f\"Label map: {label_map}\")\n",
    "else:\n",
    "    label_map = None\n",
    "    logger.info('No label map')\n",
    "templatizer = utils_v4.TriggerTemplatizer(\n",
    "    args.template,\n",
    "    config,\n",
    "    tokenizer,\n",
    "    model=args.model_name,\n",
    "    label_map=label_map,\n",
    "    label_field=args.label_field,\n",
    "    tokenize_labels=args.tokenize_labels,\n",
    "    add_special_tokens=False,\n",
    "    remove_periods=args.remove_periods,\n",
    "    replace_period_with_comma=args.replace_period_with_comma,\n",
    "    use_ctx=args.use_ctx\n",
    ")\n",
    "# Obtain the initial trigger tokens and label mapping\n",
    "if args.initial_trigger:   \n",
    "    initial_trigger = args.initial_trigger\n",
    "    logger.info(f\"initial trigger {initial_trigger}\")\n",
    "    logger.info(\"init ids\")\n",
    "    init_ids = tokenizer.convert_tokens_to_ids(initial_trigger)\n",
    "    logger.info(init_ids)\n",
    "    init_ids = torch.tensor(init_ids, device=device).unsqueeze(0)\n",
    "    logger.info(init_ids)\n",
    "    trigger_ids = tokenizer.convert_tokens_to_ids(initial_trigger)\n",
    "    logger.info(f'Initial triggers are the following: {initial_trigger}')\n",
    "    logger.info(f'Initial Trigger ids are: {trigger_ids}')\n",
    "    logger.info(f\"len trigger ids: {len(trigger_ids)}\")\n",
    "    logger.info(f\"num trigger tokens: {templatizer.num_trigger_tokens}\")\n",
    "    assert len(trigger_ids) == templatizer.num_trigger_tokens\n",
    "else:\n",
    "    logger.info(f\"no initial trigger provided, using {templatizer.num_trigger_tokens} mask tokens\")\n",
    "    init_ids = [tokenizer.mask_token_id] * templatizer.num_trigger_tokens\n",
    "    init_ids = torch.tensor(init_ids, device=device).unsqueeze(0)\n",
    "    trigger_ids = [tokenizer.mask_token_id] * templatizer.num_trigger_tokens\n",
    "trigger_ids = torch.tensor(trigger_ids, device=device).unsqueeze(0)\n",
    "best_trigger_ids = trigger_ids.clone()\n",
    "# NOTE: Accuracy can only be computed if a fixed pool of labels is given, which currently\n",
    "# requires the label map to be specified. Since producing a label map may be cumbersome (e.g.,\n",
    "# for link prediction tasks), we just use (negative) loss as the evaluation metric in these cases.\n",
    "if label_map:\n",
    "    evaluation_fn = AccuracyFn(tokenizer, label_map, device)\n",
    "else:\n",
    "    evaluation_fn = lambda x, y: -get_loss(x, y)\n",
    "logger.info('Loading datasets')\n",
    "collator = utils_v4.Collator(pad_token_id=tokenizer.pad_token_id)\n",
    "if args.perturbed:\n",
    "    train_dataset = utils_v4.load_augmented_trigger_dataset(args.train, templatizer, limit=args.limit)\n",
    "else:\n",
    "    train_dataset = utils_v4.load_trigger_dataset(args.train, templatizer, start_idx=args.start_idx, end_idx=args.end_idx, use_ctx=args.use_ctx, limit=args.limit)\n",
    "train_loader = DataLoader(train_dataset, batch_size=args.bsz, shuffle=False, collate_fn=collator)\n",
    "allowed_words = ['iPhone', 'McC', 'YouTube', 'McDonald', 'LinkedIn', 'MPs', 'WhatsApp', 'iOS', 'McCain', 'McG', 'McD', 'McConnell', 'McGregor', 'McCarthy', 'iPad', 'LeBron', 'JPMorgan', 'IoT', 'OnePlus', 'realDonaldTrump', 'BuzzFeed', 'iTunes', 'iPhones', 'SpaceX', 'McLaren', 'PhD', 'PlayStation', 'McKin', 'McCabe', 'McCoy', 'TVs', 'FedEx', 'McGr', 'McGu', 'McMahon', 'CEOs', 'McMaster', 'JavaScript', 'WikiLeaks', 'eBay', 'McKenzie', 'McInt', 'BlackBerry', 'McCorm', 'DeVos', 'PayPal', 'MacBook', 'McCull', 'PCs', 'McKay', 'MacDonald', 'McCann', 'McGee', 'NGOs', 'GHz', 'McKenna', 'McCartney', 'HuffPost', 'McGill', 'WiFi', 'McDonnell', 'iPads', 'GoPro', 'iPod', 'MacArthur', 'VMware', 'macOS', 'CDs', 'McAuliffe', 'WordPress', 'iCloud', 'YouTube', 'GeForce', 'GPUs', 'CPUs', 'GitHub', 'PowerPoint', 'eSports', 'ObamaCare', 'iPhone', 'UFOs', 'mRNA', 'StarCraft', 'LinkedIn']\n",
    "\"\"\"\n",
    "filter = torch.zeros(tokenizer.vocab_size, dtype=torch.float32, device=device)\n",
    "if args.filter:\n",
    "    logger.info('Filtering label tokens.')\n",
    "    if label_map:\n",
    "        for label_tokens in label_map.values():\n",
    "            label_ids = utils.encode_label(tokenizer, label_tokens).unsqueeze(0)\n",
    "            filter[label_ids] = 1e32\n",
    "    else:\n",
    "        for _, label_ids in train_dataset:\n",
    "            filter[label_ids] = 1e32\n",
    "    logger.info('Filtering special tokens and capitalized words.')\n",
    "    for word, idx in tokenizer.get_vocab().items():\n",
    "        if len(word) == 1 or idx >= tokenizer.vocab_size:\n",
    "            continue\n",
    "        # Filter special tokens.\n",
    "        if idx in tokenizer.all_special_ids:\n",
    "            logger.info('Filtered: %s, index: %d', word, idx)\n",
    "            filter[idx] = 1e32\n",
    "        \n",
    "        if isVariable(idx, tokenizer, allowed_words):\n",
    "            logger.debug(f\"Filtered {word}\")\n",
    "            print(word)\n",
    "            filter[idx] = 1e32\n",
    "    \n",
    "        if is_all_capps_or_num(idx, tokenizer):\n",
    "            logger.debug(f\"Filtered {word}\")\n",
    "            print(word)\n",
    "            filter[idx] = 1e32\n",
    "\n",
    "# creating the filter for the first iteration of token generation\n",
    "first_iter_filter = filter.detach().clone()\n",
    "if args.model_name == \"roberta-large\" or args.model_name == 'bert-large-cased':\n",
    "    with open(args.filtered_vocab, \"r\", encoding=\"utf-8\") as f:\n",
    "        whole_word_tokens = json.load(f)\n",
    "    for index in range(tokenizer.vocab_size):\n",
    "        if index not in whole_word_tokens.values():\n",
    "            first_iter_filter[index] = 1e32\n",
    "# end creating first iter filter\n",
    "# Save filter\n",
    "torch.save(first_iter_filter, f\"/home/zsarwar/NLP/autoprompt/data/filters/first_iter_filter_{args.model_name}.pt\")\n",
    "torch.save(filter, f\"/home/zsarwar/NLP/autoprompt/data/filters/filter_{args.model_name}.pt\")\n",
    "\"\"\"\n",
    "first_iter_filter = torch.load(f\"/home/zsarwar/NLP/autoprompt/data/filters/first_iter_filter_{args.model_name}.pt\", map_location=device)\n",
    "filter = torch.load(f\"/home/zsarwar/NLP/autoprompt/data/filters/filter_{args.model_name}.pt\", map_location=device)\n",
    "\n",
    "\n",
    "\n",
    "all_model_inputs = []\n",
    "all_labels = []\n",
    "all_pred_labels = []\n",
    "all_indices = []\n",
    "\n",
    "\n",
    "logger.info('Evaluating baseline')\n",
    "logger.info(f\"Baseline trigger ids are : {trigger_ids}\")\n",
    "numerator = 0\n",
    "numerator_acc = 0\n",
    "denominator = 0\n",
    "\n",
    "\n",
    "for idx, (model_inputs, labels) in tqdm(enumerate(train_loader)):\n",
    "\n",
    "    model_inputs = {k: v.to(device) for k, v in model_inputs.items()}\n",
    "    all_indices.append(idx)\n",
    "    labels = labels.to(device)\n",
    "    with torch.no_grad():\n",
    "        predict_logits, m_inputs = predictor(model_inputs, trigger_ids)\n",
    "    pred_label = get_pred_label(predict_logits, labels, tokenizer)\n",
    "    logger.info(f\"Index : {idx}\")\n",
    "    logger.info(f\"Input : {tokenizer.decode(m_inputs['input_ids'][0])}\")\n",
    "    logger.info(f\"Label : {tokenizer.decode(labels[0])}\")\n",
    "    logger.info(f\"Pred : {tokenizer.decode(pred_label[0])}\")\n",
    "    logger.info(f\"\\n\\n\")\n",
    "    all_model_inputs.append(m_inputs)\n",
    "    all_labels.append(labels)\n",
    "    all_pred_labels.append(pred_label)\n",
    "    numerator += evaluation_fn(predict_logits, labels).sum().item()\n",
    "    denominator += labels.size(0)\n",
    "    numerator_acc += compute_accuracy(predict_logits, labels)\n",
    "dev_metric = numerator / (denominator + 1e-13)\n",
    "acc_metric_base = numerator_acc / (denominator + 1e-13)\n",
    "logger.info(f'Dev metric: {dev_metric}')\n",
    "logger.info(f'Dev acc metric baseline is : {acc_metric_base}')\n",
    "best_dev_metric = 10\n",
    "best_dev_acc_metric = 1\n",
    "\n",
    "# precalculating the normalized embeddings\n",
    "embed_norm = torch.linalg.vector_norm(embeddings.weight, dim=1)\n",
    "normalized_embedding_weights = torch.transpose(\n",
    "    torch.divide(torch.transpose(embeddings.weight, 0, 1), embed_norm),\n",
    "    0,\n",
    "    1\n",
    ")\n",
    "if args.include_gpt:\n",
    "    # intializing GPT-2\n",
    "    gpt_model = GPT2LMHeadModel.from_pretrained('gpt2-xl')\n",
    "    gpt_tokenizer = GPT2Tokenizer.from_pretrained('gpt2-xl')\n",
    "    gpt_tokenizer.pad_token_id = gpt_tokenizer.eos_token_id\n",
    "    gpt_tokenizer.padding_side = \"left\"\n",
    "    gpt_model = gpt_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "1it [00:05,  5.46s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "1it [00:23, 23.21s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/zsarwar/NLP/autoprompt/autoprompt/trigger_v4_dev.ipynb Cell 5\u001b[0m in \u001b[0;36m<cell line: 19>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bfred.cs.uchicago.edu/home/zsarwar/NLP/autoprompt/autoprompt/trigger_v4_dev.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=103'>104</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bfred.cs.uchicago.edu/home/zsarwar/NLP/autoprompt/autoprompt/trigger_v4_dev.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=104'>105</a>\u001b[0m     set_seed(curr_attempt)\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2Bfred.cs.uchicago.edu/home/zsarwar/NLP/autoprompt/autoprompt/trigger_v4_dev.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=105'>106</a>\u001b[0m     gpt_outputs \u001b[39m=\u001b[39m gpt_model\u001b[39m.\u001b[39;49mgenerate(inputs\u001b[39m=\u001b[39;49mgpt_encoded_prompts[\u001b[39m'\u001b[39;49m\u001b[39minput_ids\u001b[39;49m\u001b[39m'\u001b[39;49m], attention_mask\u001b[39m=\u001b[39;49mgpt_encoded_prompts[\u001b[39m'\u001b[39;49m\u001b[39mattention_mask\u001b[39;49m\u001b[39m'\u001b[39;49m], do_sample\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, top_p\u001b[39m=\u001b[39;49m\u001b[39m0.96\u001b[39;49m, output_scores\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, return_dict_in_generate\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, max_length\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bfred.cs.uchicago.edu/home/zsarwar/NLP/autoprompt/autoprompt/trigger_v4_dev.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=106'>107</a>\u001b[0m num_tokens \u001b[39m=\u001b[39m gpt_encoded_prompts[\u001b[39m'\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mnumel()\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bfred.cs.uchicago.edu/home/zsarwar/NLP/autoprompt/autoprompt/trigger_v4_dev.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=107'>108</a>\u001b[0m \u001b[39m# Need Entire GPT-2 Text here for entire text\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/nlp2/lib/python3.10/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/nlp2/lib/python3.10/site-packages/transformers/generation_utils.py:1351\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, max_length, min_length, do_sample, early_stopping, num_beams, temperature, top_k, top_p, typical_p, repetition_penalty, bad_words_ids, force_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, encoder_no_repeat_ngram_size, num_return_sequences, max_time, max_new_tokens, decoder_start_token_id, use_cache, num_beam_groups, diversity_penalty, prefix_allowed_tokens_fn, logits_processor, renormalize_logits, stopping_criteria, constraints, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, forced_bos_token_id, forced_eos_token_id, remove_invalid_values, synced_gpus, exponential_decay_length_penalty, **model_kwargs)\u001b[0m\n\u001b[1;32m   1343\u001b[0m     input_ids, model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1344\u001b[0m         input_ids,\n\u001b[1;32m   1345\u001b[0m         expand_size\u001b[39m=\u001b[39mnum_return_sequences,\n\u001b[1;32m   1346\u001b[0m         is_encoder_decoder\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1347\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1348\u001b[0m     )\n\u001b[1;32m   1350\u001b[0m     \u001b[39m# 12. run sample\u001b[39;00m\n\u001b[0;32m-> 1351\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msample(\n\u001b[1;32m   1352\u001b[0m         input_ids,\n\u001b[1;32m   1353\u001b[0m         logits_processor\u001b[39m=\u001b[39;49mlogits_processor,\n\u001b[1;32m   1354\u001b[0m         logits_warper\u001b[39m=\u001b[39;49mlogits_warper,\n\u001b[1;32m   1355\u001b[0m         stopping_criteria\u001b[39m=\u001b[39;49mstopping_criteria,\n\u001b[1;32m   1356\u001b[0m         pad_token_id\u001b[39m=\u001b[39;49mpad_token_id,\n\u001b[1;32m   1357\u001b[0m         eos_token_id\u001b[39m=\u001b[39;49meos_token_id,\n\u001b[1;32m   1358\u001b[0m         output_scores\u001b[39m=\u001b[39;49moutput_scores,\n\u001b[1;32m   1359\u001b[0m         return_dict_in_generate\u001b[39m=\u001b[39;49mreturn_dict_in_generate,\n\u001b[1;32m   1360\u001b[0m         synced_gpus\u001b[39m=\u001b[39;49msynced_gpus,\n\u001b[1;32m   1361\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs,\n\u001b[1;32m   1362\u001b[0m     )\n\u001b[1;32m   1364\u001b[0m \u001b[39melif\u001b[39;00m is_beam_gen_mode:\n\u001b[1;32m   1365\u001b[0m     \u001b[39mif\u001b[39;00m num_return_sequences \u001b[39m>\u001b[39m num_beams:\n",
      "File \u001b[0;32m~/.conda/envs/nlp2/lib/python3.10/site-packages/transformers/generation_utils.py:1967\u001b[0m, in \u001b[0;36mGenerationMixin.sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   1964\u001b[0m model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   1966\u001b[0m \u001b[39m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 1967\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(\n\u001b[1;32m   1968\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_inputs,\n\u001b[1;32m   1969\u001b[0m     return_dict\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   1970\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1971\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1972\u001b[0m )\n\u001b[1;32m   1974\u001b[0m \u001b[39mif\u001b[39;00m synced_gpus \u001b[39mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   1975\u001b[0m     cur_len \u001b[39m=\u001b[39m cur_len \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/.conda/envs/nlp2/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/nlp2/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:1063\u001b[0m, in \u001b[0;36mGPT2LMHeadModel.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1055\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1056\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1057\u001b[0m \u001b[39m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[1;32m   1058\u001b[0m \u001b[39m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[1;32m   1059\u001b[0m \u001b[39m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[1;32m   1060\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1061\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1063\u001b[0m transformer_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer(\n\u001b[1;32m   1064\u001b[0m     input_ids,\n\u001b[1;32m   1065\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   1066\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1067\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[1;32m   1068\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m   1069\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   1070\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   1071\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m   1072\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[1;32m   1073\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   1074\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1075\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1076\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1077\u001b[0m )\n\u001b[1;32m   1078\u001b[0m hidden_states \u001b[39m=\u001b[39m transformer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1080\u001b[0m \u001b[39m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/nlp2/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/nlp2/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:906\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    896\u001b[0m     outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    897\u001b[0m         create_custom_forward(block),\n\u001b[1;32m    898\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    903\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    904\u001b[0m     )\n\u001b[1;32m    905\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 906\u001b[0m     outputs \u001b[39m=\u001b[39m block(\n\u001b[1;32m    907\u001b[0m         hidden_states,\n\u001b[1;32m    908\u001b[0m         layer_past\u001b[39m=\u001b[39;49mlayer_past,\n\u001b[1;32m    909\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    910\u001b[0m         head_mask\u001b[39m=\u001b[39;49mhead_mask[i],\n\u001b[1;32m    911\u001b[0m         encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m    912\u001b[0m         encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[1;32m    913\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    914\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    915\u001b[0m     )\n\u001b[1;32m    917\u001b[0m hidden_states \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    918\u001b[0m \u001b[39mif\u001b[39;00m use_cache \u001b[39mis\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/nlp2/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/nlp2/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:406\u001b[0m, in \u001b[0;36mGPT2Block.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    404\u001b[0m residual \u001b[39m=\u001b[39m hidden_states\n\u001b[1;32m    405\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_1(hidden_states)\n\u001b[0;32m--> 406\u001b[0m attn_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattn(\n\u001b[1;32m    407\u001b[0m     hidden_states,\n\u001b[1;32m    408\u001b[0m     layer_past\u001b[39m=\u001b[39;49mlayer_past,\n\u001b[1;32m    409\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    410\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    411\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    412\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    413\u001b[0m )\n\u001b[1;32m    414\u001b[0m attn_output \u001b[39m=\u001b[39m attn_outputs[\u001b[39m0\u001b[39m]  \u001b[39m# output_attn: a, present, (attentions)\u001b[39;00m\n\u001b[1;32m    415\u001b[0m outputs \u001b[39m=\u001b[39m attn_outputs[\u001b[39m1\u001b[39m:]\n",
      "File \u001b[0;32m~/.conda/envs/nlp2/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/nlp2/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:347\u001b[0m, in \u001b[0;36mGPT2Attention.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    345\u001b[0m     attn_output, attn_weights \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_upcast_and_reordered_attn(query, key, value, attention_mask, head_mask)\n\u001b[1;32m    346\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 347\u001b[0m     attn_output, attn_weights \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_attn(query, key, value, attention_mask, head_mask)\n\u001b[1;32m    349\u001b[0m attn_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_merge_heads(attn_output, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhead_dim)\n\u001b[1;32m    350\u001b[0m attn_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mc_proj(attn_output)\n",
      "File \u001b[0;32m~/.conda/envs/nlp2/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:212\u001b[0m, in \u001b[0;36mGPT2Attention._attn\u001b[0;34m(self, query, key, value, attention_mask, head_mask)\u001b[0m\n\u001b[1;32m    209\u001b[0m     mask_value \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfinfo(attn_weights\u001b[39m.\u001b[39mdtype)\u001b[39m.\u001b[39mmin\n\u001b[1;32m    210\u001b[0m     \u001b[39m# Need to be a tensor, otherwise we get error: `RuntimeError: expected scalar type float but found double`.\u001b[39;00m\n\u001b[1;32m    211\u001b[0m     \u001b[39m# Need to be on the same device, otherwise `RuntimeError: ..., x and y to be on the same device`\u001b[39;00m\n\u001b[0;32m--> 212\u001b[0m     mask_value \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mtensor(mask_value, dtype\u001b[39m=\u001b[39;49mattn_weights\u001b[39m.\u001b[39;49mdtype)\u001b[39m.\u001b[39;49mto(attn_weights\u001b[39m.\u001b[39;49mdevice)\n\u001b[1;32m    213\u001b[0m     attn_weights \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mwhere(causal_mask, attn_weights, mask_value)\n\u001b[1;32m    215\u001b[0m \u001b[39mif\u001b[39;00m attention_mask \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    216\u001b[0m     \u001b[39m# Apply the attention mask\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "all_model_inputs_triggers = []\n",
    "all_labels_triggers = []\n",
    "all_pred_labels_triggers = []\n",
    "all_gpt_encodings = []\n",
    "all_gpt_generations = []\n",
    "all_model_encodings = []\n",
    "all_adv_tokens = []\n",
    "all_indices_triggers = []\n",
    "all_sub_indices_triggers = []\n",
    "all_first_success_ranks = []\n",
    "\n",
    "all_losses = []\n",
    "new_example = True\n",
    "total_samples = 0\n",
    "total_incorrect = 0\n",
    "model.zero_grad()\n",
    "averaged_grad = None\n",
    "# Accumulate\n",
    "for idx, (model_inputs, labels) in tqdm(enumerate(train_loader)):\n",
    "    logger.info(f\"Total successes  : {total_incorrect}\")\n",
    "    curr_losses = []\n",
    "    new_example=True\n",
    "    total_samples+=1    \n",
    "    # Start from scratch for each example\n",
    "    all_indices_triggers.append(idx)\n",
    "    trigger_ids = init_ids.clone()\n",
    "    model.zero_grad()\n",
    "    model_inputs = {k: v.to(device) for k, v in model_inputs.items()}\n",
    "    labels = labels.to(device)\n",
    "    with torch.no_grad():   \n",
    "        predict_logits, _ = predictor(model_inputs, trigger_ids)\n",
    "        eval_metric = evaluation_fn(predict_logits, labels)\n",
    "        eval_acc_metric = compute_accuracy(predict_logits, labels)\n",
    "    for token_to_flip in range(templatizer.num_trigger_tokens):\n",
    "        model.zero_grad()\n",
    "        predict_logits, _ = predictor(model_inputs, trigger_ids)\n",
    "        loss = get_loss(predict_logits, labels).mean()\n",
    "        curr_losses.append(-loss.unsqueeze(0))\n",
    "        loss.backward()\n",
    "        grad = embedding_gradient.get()\n",
    "        bsz, _, emb_dim = grad.size()\n",
    "        selection_mask = model_inputs['trigger_mask'].unsqueeze(-1)\n",
    "        grad = torch.masked_select(grad, selection_mask)\n",
    "        grad = grad.view(bsz, templatizer.num_trigger_tokens, emb_dim)\n",
    "        averaged_grad = grad.sum(dim=0)\n",
    "        all_labels_triggers.append(labels)\n",
    "        # Compute adv tokens in any case    \n",
    "        candidates = hotflip_attack(averaged_grad[token_to_flip],\n",
    "                                    normalized_embedding_weights,\n",
    "                                    increase_loss=True,\n",
    "                                    num_candidates=args.num_cand,\n",
    "                                    filter=filter if token_to_flip > 0 else first_iter_filter)\n",
    "            \n",
    "        all_adv_tokens.append(candidates)\n",
    "        current_score = 0\n",
    "        current_acc = 0\n",
    "        candidate_scores = torch.zeros(args.num_cand, device=device)\n",
    "        candidate_accs = torch.zeros(args.num_cand, device=device)\n",
    "        candidate_pred_labels = torch.zeros(args.num_cand, device=device, dtype=int)\n",
    "        denom = 0\n",
    "        all_candidates = []\n",
    "        entire_text = []\n",
    "        # Update current score\n",
    "        current_acc = eval_acc_metric\n",
    "        current_score = eval_metric.sum()\n",
    "        denom = labels.size(0)    \n",
    "        # Changes start from here\n",
    "        # Batched og prompts in id form\n",
    "        original_prompt_ids = model_inputs['input_ids'][0].unsqueeze(0)\n",
    "        original_prompt_ids = original_prompt_ids.repeat(args.num_cand, 1)\n",
    "        rep_token_idx = torch.where(original_prompt_ids == tokenizer.mask_token_id)[1][0]\n",
    "        original_prompt_ids[:,rep_token_idx] = labels[0].item()\n",
    "        # Batched trigger prompts in id form\n",
    "        temp_trigger = trigger_ids.clone()\n",
    "        temp_triggers = temp_trigger.repeat(len(candidates), 1)\n",
    "        temp_triggers[:, token_to_flip] = candidates\n",
    "        # Batched og + trigger prompts in text form\n",
    "        original_prompts = tokenizer.batch_decode(original_prompt_ids, skip_special_tokens=True)\n",
    "        candidates_strs = tokenizer.batch_decode(candidates.unsqueeze(1))\n",
    "        if(args.include_adv_token):\n",
    "            if(\"roberta\" in args.model_name):\n",
    "                pre_text = [candidates_strs[i] + original_prompts[i] for i in range(len(original_prompts))]\n",
    "            elif(\"bert\" in args.model_name):\n",
    "                pre_text = [original_prompts[i] + \" \" + candidates_strs[i] for i in range(len(original_prompts))]    \n",
    "        else:\n",
    "            pre_text = [original_prompts[i] for i in range(len(original_prompts))]\n",
    "\n",
    "        skip_indices = []\n",
    "        curr_attempt = 0\n",
    "        found_adv_gen = False\n",
    "        non_gpt_session=False\n",
    "        \n",
    "        batch_gpt_encoded_prompts = []\n",
    "        batch_gpt_tokens = []\n",
    "        batch_curr_inputs = []\n",
    "        batch_curr_pred_labels = []\n",
    "        batch_sub_indices = []\n",
    "        while(curr_attempt < args.tot_gpt_attempts and not found_adv_gen):\n",
    "            if(args.include_gpt):\n",
    "                # Encode for GPT-2 Generations and generate\n",
    "                gpt_encoded_prompts = gpt_tokenizer.batch_encode_plus(pre_text, add_special_tokens=True, return_attention_mask=True, padding='longest', return_tensors='pt').to(device) \n",
    "                batch_gpt_encoded_prompts.append(gpt_encoded_prompts)\n",
    "                #all_gpt_encodings.append(gpt_encoded_prompts)\n",
    "                with torch.no_grad():\n",
    "                    set_seed(curr_attempt)\n",
    "                    gpt_outputs = gpt_model.generate(inputs=gpt_encoded_prompts['input_ids'], attention_mask=gpt_encoded_prompts['attention_mask'], do_sample=True, top_p=0.96, output_scores=False, return_dict_in_generate=True, max_length=100)\n",
    "                num_tokens = gpt_encoded_prompts['input_ids'][0].numel()\n",
    "                # Need Entire GPT-2 Text here for entire text\n",
    "                gpt_all_tokens = gpt_outputs['sequences']\n",
    "                batch_gpt_tokens+=gpt_all_tokens\n",
    "                #all_gpt_generations.append(gpt_all_tokens)\n",
    "                gpt_all_tokens_str = gpt_tokenizer.batch_decode(gpt_all_tokens, skip_special_tokens=True)\n",
    "                #NLTK for all_tokens    \n",
    "                gpt_all_str_sents = [tokenize.sent_tokenize(sent) for sent in gpt_all_tokens_str]\n",
    "                \n",
    "                if(args.remove_periods or args.replace_period_with_comma):\n",
    "                    gpt_gen_with_og = [all_sents[0] for all_sents in gpt_all_str_sents]\n",
    "                else:\n",
    "                    gpt_gen_with_og = [' '.join(all_sents[0:2]) for all_sents in gpt_all_str_sents]             \n",
    "                entire_text = gpt_gen_with_og\n",
    "                # Separate the newly generated tokens\n",
    "                gpt_new_tokens = gpt_outputs['sequences'][:, num_tokens:]\n",
    "                gpt_new_tokens_str = gpt_tokenizer.batch_decode(gpt_new_tokens, skip_special_tokens=True)\n",
    "                # NLTK for new_tokens\n",
    "                gpt_new_str_sents = [tokenize.sent_tokenize(sent) for sent in gpt_new_tokens_str]\n",
    "                gpt_gen = [all_sents[0] if(len(all_sents) >= 1 ) else \"SKIPPING\" for all_sents in gpt_new_str_sents]\n",
    "                skip_indices = [i for i, sent in enumerate(gpt_gen) if sent == \"SKIPPING\"]\n",
    "                # Maybe add length check?\n",
    "            elif(args.include_adv_token):\n",
    "                skip_indices = []\n",
    "                entire_text = pre_text\n",
    "            #TODO Add condition for wikipedia        \n",
    "            # Retokenize trigger tokens into bert with adv. tokens or gpt_generations or wikipedia text or any combination of them.\n",
    "            # Everything needs to be in text here    \n",
    "            # Insert trigger token in the beginning\n",
    "            if(args.include_adv_token):\n",
    "                if(args.include_gpt):\n",
    "                    if(\"roberta\" in args.model_name):\n",
    "                        final_trigg_text = [candidates_strs[i] + gpt_gen[i] for i in range(len(gpt_gen))]\n",
    "                    elif(\"bert\" in args.model_name):\n",
    "                        final_trigg_text = [\" \" + candidates_strs[i] + gpt_gen[i] for i in range(len(gpt_gen))]   \n",
    "                else:\n",
    "                    if(\"roberta\" in args.model_name):\n",
    "                        final_trigg_text = candidates_strs\n",
    "                    elif(\"bert\" in args.model_name):\n",
    "                        final_trigg_text = [\" \" + candidates_strs[i] for i in range(len(candidates_strs))]\n",
    "                        # Only so that non-GPT setting does not have to loop\n",
    "            elif(args.include_gpt):\n",
    "                if(\"roberta\" in args.model_name):\n",
    "                    final_trigg_text = gpt_gen\n",
    "                elif(\"bert\" in args.model_name):\n",
    "                    final_trigg_text = [\" \" + gpt_gen[i] for i in range(len(gpt_gen))]\n",
    "            # Tokenize trigger text into to-be-attacked models token ids\n",
    "            final_trigg_tokens = tokenizer.batch_encode_plus(final_trigg_text, add_special_tokens=False)\n",
    "            all_candidates = final_trigg_tokens['input_ids']\n",
    "            # Evaluate with adversarial prompts\n",
    "            curr_inputs = []\n",
    "            curr_pred_labels = []\n",
    "            sub_indices = []\n",
    "            for j in range(len(all_candidates)): \n",
    "                if j not in skip_indices:\n",
    "                    trigg_toks = torch.tensor(all_candidates[j], device=device).unsqueeze(0)\n",
    "                    with torch.no_grad():\n",
    "                        predict_logits, m_inpts = predictor(model_inputs, trigg_toks)\n",
    "                        eval_metric = evaluation_fn(predict_logits, labels)\n",
    "                        curr_losses.append(eval_metric)\n",
    "                        pred_label= get_pred_label(predict_logits, labels, tokenizer)\n",
    "                        eval_attack_acc_metric = compute_accuracy(predict_logits, labels)\n",
    "                    curr_inputs.append(m_inpts)\n",
    "                    curr_pred_labels.append(pred_label)\n",
    "                    candidate_scores[j] = eval_metric.sum()\n",
    "                    candidate_accs[j] = eval_attack_acc_metric\n",
    "                    candidate_pred_labels[j] = pred_label\n",
    "                else:\n",
    "                    logger.info(\"Skipping because of empty generation sequence\")\n",
    "                    curr_inputs.append(-100)\n",
    "                    curr_pred_labels.append(-100)\n",
    "            batch_curr_inputs+=curr_inputs\n",
    "            batch_curr_pred_labels+=curr_pred_labels\n",
    "            # Print and save successful prompts\n",
    "            \n",
    "            logger.info(f\"Batch  : {curr_attempt}\")\n",
    "            all_cands = torch.where(candidate_accs == 0)[0]\n",
    "            true_cands = torch.tensor([cand for cand in all_cands if cand not in skip_indices], device=device)\n",
    "            if(true_cands.shape[0] >= 1):    \n",
    "                first_succ_idx = true_cands[0]\n",
    "                logger.info(f\"Index  : {idx}\")\n",
    "                logger.info(f\"Original  : {original_prompts[0]}\")\n",
    "                real_label = tokenizer.convert_ids_to_tokens(labels)\n",
    "                for index in true_cands:\n",
    "                    sub_indices.append(index)\n",
    "                    if(not found_adv_gen):\n",
    "                        total_incorrect+=1\n",
    "                        all_first_success_ranks.append(first_succ_idx)\n",
    "                        found_adv_gen = True\n",
    "                        if(not args.include_gpt):\n",
    "                            non_gpt_session = True\n",
    "\n",
    "                    adv_lab = candidate_pred_labels[index].item()\n",
    "                    #Replace only the first instance of the true label with the predicted (adversarial) label\n",
    "                    encoded_entire_text = tokenizer.encode(entire_text[index])\n",
    "                    encoded_entire_text[rep_token_idx] = adv_lab\n",
    "                    entire_text[index] = tokenizer.decode(encoded_entire_text, skip_special_tokens=True)\n",
    "                    adv_text_pred = entire_text[index]\n",
    "                    trigger_ids = all_candidates[index]\n",
    "                    logger.info(f\"Adversarial {index + 10*curr_attempt}: {adv_text_pred}\")       \n",
    "                logger.info(f\"\\n\\n\")\n",
    "                #all_sub_indices_triggers.append(sub_indices)\n",
    "                batch_sub_indices+=sub_indices\n",
    "                break\n",
    "            else:\n",
    "                curr_attempt+=1\n",
    "                if(curr_attempt == args.tot_gpt_attempts -1):\n",
    "                    all_first_success_ranks.append(-100)\n",
    "                if(found_adv_gen or non_gpt_session or not args.include_gpt):\n",
    "                    break    \n",
    "        # GPT_Generations finished, now appending batched inputs\n",
    "        all_gpt_encodings.append(batch_gpt_encoded_prompts)\n",
    "        all_gpt_generations.append(batch_gpt_tokens)\n",
    "        all_model_inputs_triggers.append(batch_curr_inputs)\n",
    "        all_pred_labels_triggers.append(batch_curr_pred_labels)\n",
    "        all_sub_indices_triggers.append(batch_sub_indices)\n",
    "        all_losses.append(curr_losses)\n",
    "        break\n",
    "flip_rate = total_incorrect / total_samples + 1e-32\n",
    "logger.info(f\"Total incorrect are : {total_incorrect}\")\n",
    "logger.info(f\"Total samples are : {total_samples}\")\n",
    "logger.info(f\"Flip rate is : {flip_rate}\")\n",
    "# Saving results\n",
    "all_results_dict = {}\n",
    "results_baseline = {}\n",
    "results_baseline['all_model_inputs'] = all_model_inputs\n",
    "results_baseline['all_labels'] = all_labels\n",
    "results_baseline['all_pred_labels'] = all_pred_labels\n",
    "results_baseline['all_indices'] = all_indices\n",
    "all_results_dict['results_baseline'] = results_baseline\n",
    "results_adversarial = {}\n",
    "results_adversarial['all_model_inputs_triggers'] = all_model_inputs_triggers\n",
    "results_adversarial['all_labels_triggers'] = all_labels_triggers\n",
    "results_adversarial['all_pred_labels_triggers'] = all_pred_labels_triggers\n",
    "results_adversarial['all_gpt_encodings'] = all_gpt_encodings\n",
    "results_adversarial['all_gpt_generations'] = all_gpt_generations\n",
    "results_adversarial['all_model_encodings'] = all_model_encodings\n",
    "results_adversarial['all_adv_tokens'] = all_adv_tokens\n",
    "results_adversarial['all_indices_triggers'] = all_indices_triggers\n",
    "results_adversarial['all_sub_indices_triggers'] = all_sub_indices_triggers\n",
    "results_adversarial['all_first_success_ranks'] = all_first_success_ranks\n",
    "results_adversarial['all_losses'] = all_losses\n",
    "all_results_dict['results_adversarial'] = results_adversarial\n",
    "np.save(numpy_file, all_results_dict, allow_pickle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('nlp2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "37239dad5b8b56b0d6556e681c63cac8a09607160e7510acb3444e24aad22957"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
