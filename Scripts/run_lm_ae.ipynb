{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "from transformers import RobertaTokenizer, RobertaForMaskedLM\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, Dataset, DataLoader, SequentialSampler\n",
    "import os\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import jsonlines as js\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preparedata(infile):\n",
    "    sentences = []\n",
    "    labels = []\n",
    "    \n",
    "    f = open(inp_file)\n",
    "    all_lines = f.readlines()\n",
    "    for l in all_lines[1:]:\n",
    "        temp_s = ''.join(l.split('\\t')[0]).strip()\n",
    "        temp_s = temp_s.replace(\"[MASK]\", \"<mask>\")\n",
    "        temp_l = ''.join(l.split('\\t')[-1]).strip()\n",
    "        temp_l = \" \" + temp_l\n",
    "        sentences.append(temp_s)\n",
    "        labels.append(temp_l)\n",
    "    \n",
    "\n",
    "    return sentences, labels\n",
    "\n",
    "\n",
    "def prep_inputs(sents, tokenizer):\n",
    "    \n",
    "    mask_token_indices = []\n",
    "    batch_input_ids = tokenizer.batch_encode_plus(sents, add_special_tokens=True, padding=True, return_attention_mask=True, return_tensors='pt')\n",
    "\n",
    "    for i, inp_ids in enumerate(batch_input_ids['input_ids']):\n",
    "        \n",
    "        mask_index = (inp_ids == tokenizer.mask_token_id).nonzero(as_tuple=True)[0]\n",
    "        mask_index = torch.where(inp_ids == tokenizer.mask_token_id)[0]\n",
    "        mask_token_indices.append(mask_index)\n",
    "    \n",
    "\n",
    "    return batch_input_ids, torch.tensor(mask_token_indices)\n",
    "\n",
    "\n",
    "def get_predictions(model, dataloader):  \n",
    "    all_correct = 0\n",
    "    tot_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "            all_correct_samples_mask = []\n",
    "            all_correct_labels = []\n",
    "            for i, batch in enumerate(eval_dataloader):\n",
    "                for x in range(len(batch)):\n",
    "                    batch[x] = batch[x].to(device)\n",
    "                logits = model(input_ids= batch[0],attention_mask = batch[1]).logits\n",
    "                soft_preds = torch.nn.functional.softmax(logits, dim=-1)\n",
    "                pred_token_ids = torch.tensor([soft_preds[i, batch[2][i]].argmax(axis=-1) for i in range(soft_preds.shape[0])], device=device)\n",
    "                \n",
    "                tot_correct = torch.eq(pred_token_ids, batch[3]).count_nonzero().item()\n",
    "                corr_mask = torch.eq(pred_token_ids, batch[3])\n",
    "                labs = batch[3]\n",
    "                correct_labels = tokenizer.convert_ids_to_tokens(torch.masked_select(labs.unsqueeze(-1), corr_mask.unsqueeze(-1)))\n",
    "                corr_mask = corr_mask.detach().tolist()\n",
    "                all_correct_samples_mask += corr_mask\n",
    "                all_correct += tot_correct\n",
    "                all_correct_labels += correct_labels\n",
    "                tot_samples+= batch[0].shape[0]\n",
    "                \n",
    "            acc = all_correct / tot_samples\n",
    "            print(f\"Total samples : {tot_samples}\")\n",
    "            print(f\"Correctly predicted : {all_correct}\")\n",
    "            \n",
    "            return acc, all_correct_samples_mask, all_correct_labels\n",
    "                \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "inp_file = \"/home/zsarwar/NLP/Sorting-Through-The-Noise/data/Varying_key_entity/test.csv\"\n",
    "\n",
    "sentences, labels = preparedata(inp_file)\n",
    "labels = np.asarray(labels)\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-large\", cache_dir='/bigstor/zsarwar/models/cache/')\n",
    "labels_tok_indices = torch.tensor([tokenizer(lab, return_attention_mask=False, add_special_tokens=False, return_token_type_ids=False)['input_ids'][0]  for lab in labels])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 7716, 11121,  8084,  ...,   724,   422,  3799])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_tok_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fl']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens([4825])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "input_ids, mask_token_indices = prep_inputs(sentences, tokenizer)\n",
    "eval_dataset = TensorDataset(input_ids['input_ids'],input_ids['attention_mask'], mask_token_indices, labels_tok_indices)\n",
    "eval_dataloader = DataLoader(eval_dataset, sampler = SequentialSampler(eval_dataset), batch_size= 64, drop_last=True)\n",
    "model = RobertaForMaskedLM.from_pretrained('roberta-large', cache_dir='bigstor/zsarwar/models/cache')\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples : 16768\n",
      "Correctly predicted : 8761\n"
     ]
    }
   ],
   "source": [
    "\n",
    "acc, correctly_classified_mask, correctly_classified_labels = get_predictions(model, eval_dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = np.asarray(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_short = sentences[0:len(correctly_classified_mask)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_class_sent = sentences_short[correctly_classified_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_class_sent = corr_class_sent.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with js.open(\"/home/zsarwar/NLP/Sorting-Through-The-Noise/data/Varying_key_entity/Correctly_classified_roberta_large.jsonl\", 'w') as out_file:\n",
    "    for i, sample in enumerate(corr_class_sent):\n",
    "        out = {\"Index\" : i, \"Text\" : sample, \"Label\" : correctly_classified_labels[i]}\n",
    "        out_file.write(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('conda_env_2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3561554edce98395dff0374c85b70e51cb1afebe2ecc296e0c1c886ecb81ec48"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
