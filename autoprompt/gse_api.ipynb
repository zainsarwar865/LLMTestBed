{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googlesearch import search\n",
    "from newspaper import Article\n",
    "from newsfetch.news import newspaper\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import jsonlines\n",
    "from googleapiclient.discovery import build\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from urllib.parse import urlparse, urljoin\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import jsonlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = \"AIzaSyAC2fKvrczedZhBfZmSmYDRaYvSF_nm5HU\"\n",
    "SEID = \"017236688157949926015:-2mk2err3ps\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all the facts\n",
    "all_facts = []\n",
    "with jsonlines.open(\"/home/zsarwar/NLP/autoprompt/data/correctly_classified_roberta_large_autoprompt_format_short.jsonl\", 'r') as in_file:\n",
    "    for fact in in_file:\n",
    "        af = fact['Pre_Mask'] + fact['Label'] + fact['Post_Mask'].replace(\" .\", \".\")\n",
    "        all_facts.append(af)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def google_search_api(search_term, api_key, cse_id, num_results,num_iters,\n",
    "                       **kwargs):\n",
    "    start_index = -9\n",
    "    results = []\n",
    "    for i in range(num_iters):\n",
    "        if i == (num_iters - 1):\n",
    "            if num_iters == 10:\n",
    "                num_results = (num_results - 1)\n",
    "        start_index += 10\n",
    "        service = build(\"customsearch\", \"v1\", developerKey=api_key)\n",
    "        res = service.cse().list(q=search_term, cx=cse_id,num = num_results,start = start_index , **kwargs).execute()\n",
    "        results.append(res)\n",
    "    return results\n",
    "def get_urls_api(search_results):\n",
    "    extracted_urls = set()\n",
    "    for api_results in search_results:\n",
    "        for page in api_results:\n",
    "            if (page.get('items') is not None):\n",
    "                for i in range(len(page['items'])):\n",
    "                    ex = page['items'][i]['link']\n",
    "                    extracted_urls.add(ex)\n",
    "    return extracted_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [00:00<00:03,  1.05it/s]not a 200 response: 403\n",
      "100%|██████████| 5/5 [00:12<00:00,  2.57s/it]\n",
      "100%|██████████| 5/5 [00:10<00:00,  2.12s/it]\n",
      " 60%|██████    | 3/5 [00:07<00:04,  2.33s/it]not a 200 response: 999\n",
      " 80%|████████  | 4/5 [00:09<00:01,  1.88s/it]not a 200 response: 999\n",
      "100%|██████████| 5/5 [00:10<00:00,  2.04s/it]\n",
      " 40%|████      | 2/5 [00:05<00:08,  2.76s/it]not a 200 response: 999\n",
      " 60%|██████    | 3/5 [00:06<00:03,  1.94s/it]not a 200 response: 999\n",
      "100%|██████████| 5/5 [00:08<00:00,  1.76s/it]\n"
     ]
    }
   ],
   "source": [
    "out_file = jsonlines.open(\"Testing_stuff.jsonl\", 'a')\n",
    "\n",
    "for fact in all_facts[0:4]:\n",
    "  search_results = []\n",
    "  api_results =  google_search_api(search_term = fact, api_key = API_KEY,\n",
    "                                  cse_id = SEID,\n",
    "                                  num_results = 5,num_iters = 1)\n",
    "  search_results.append(api_results)\n",
    "  extracted_urls = get_urls_api(search_results)\n",
    "  # Push wikipedia to the top (if it exists)\n",
    "  extracted_urls = list(extracted_urls)\n",
    "\n",
    "  for i, url in enumerate(extracted_urls):\n",
    "    if \"wikipedia\" in url:\n",
    "      extracted_urls.insert(0, extracted_urls.pop(i))\n",
    "  \n",
    "  for link in tqdm(extracted_urls):\n",
    "    news = newspaper(link)\n",
    "    if (len(news.article.split(' ')) <= 100):\n",
    "      continue\n",
    "    else:\n",
    "      art = news.get_dict['article']\n",
    "      # Cleaning the article\n",
    "      art_split = art.split('.')\n",
    "      art_len = len(art_split) - 1\n",
    "      for i, sent in enumerate(reversed(art_split)):\n",
    "          if('disambiguation' in sent.lower() or 'redirects' in sent.lower()):\n",
    "              art_split.pop(art_len - i)      \n",
    "      art = '. '.join(art_split)\n",
    "      # Removing wikipedia based artifacts\n",
    "      art = re.sub(' ?\\[[0-9]*\\]', \"\", art)\n",
    "      art = re.sub('\\..{1,40}\\[ edit \\]', \".\", art)\n",
    "      art = re.sub('\\[ edit \\]', \"\", art)\n",
    "      out_dict = {\"Fact\" : fact, \"URL\" : link, 'article': art}\n",
    "      out_file.write(out_dict)\n",
    "out_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('nlp2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "37239dad5b8b56b0d6556e681c63cac8a09607160e7510acb3444e24aad22957"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
